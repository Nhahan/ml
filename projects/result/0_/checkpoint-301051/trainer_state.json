{
  "best_metric": 0.045076675713062286,
  "best_model_checkpoint": "./result/0_\\checkpoint-301051",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 301051,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016608481619393393,
      "grad_norm": 0.1996103674173355,
      "learning_rate": 4.999999669155866e-05,
      "loss": 0.7183,
      "step": 500
    },
    {
      "epoch": 0.0033216963238786785,
      "grad_norm": 0.13096952438354492,
      "learning_rate": 4.999998657766518e-05,
      "loss": 0.0956,
      "step": 1000
    },
    {
      "epoch": 0.004982544485818018,
      "grad_norm": 0.11438388377428055,
      "learning_rate": 4.999996965765531e-05,
      "loss": 0.0875,
      "step": 1500
    },
    {
      "epoch": 0.006643392647757357,
      "grad_norm": 0.14187999069690704,
      "learning_rate": 4.999994593153365e-05,
      "loss": 0.0826,
      "step": 2000
    },
    {
      "epoch": 0.008304240809696696,
      "grad_norm": 0.1290454864501953,
      "learning_rate": 4.999991539930668e-05,
      "loss": 0.0806,
      "step": 2500
    },
    {
      "epoch": 0.009965088971636036,
      "grad_norm": 0.1829233467578888,
      "learning_rate": 4.99998780609827e-05,
      "loss": 0.0783,
      "step": 3000
    },
    {
      "epoch": 0.011625937133575374,
      "grad_norm": 0.19780246913433075,
      "learning_rate": 4.999983391657188e-05,
      "loss": 0.0756,
      "step": 3500
    },
    {
      "epoch": 0.013286785295514714,
      "grad_norm": 0.2261611372232437,
      "learning_rate": 4.9999782966086226e-05,
      "loss": 0.0743,
      "step": 4000
    },
    {
      "epoch": 0.014947633457454052,
      "grad_norm": 0.12472386658191681,
      "learning_rate": 4.999972520953962e-05,
      "loss": 0.0736,
      "step": 4500
    },
    {
      "epoch": 0.016608481619393392,
      "grad_norm": 0.1493726670742035,
      "learning_rate": 4.999966064694778e-05,
      "loss": 0.0714,
      "step": 5000
    },
    {
      "epoch": 0.018269329781332732,
      "grad_norm": 0.22630605101585388,
      "learning_rate": 4.999958927832828e-05,
      "loss": 0.0702,
      "step": 5500
    },
    {
      "epoch": 0.019930177943272072,
      "grad_norm": 0.0942348837852478,
      "learning_rate": 4.999951110370057e-05,
      "loss": 0.0694,
      "step": 6000
    },
    {
      "epoch": 0.02159102610521141,
      "grad_norm": 0.149961456656456,
      "learning_rate": 4.999942612308591e-05,
      "loss": 0.0698,
      "step": 6500
    },
    {
      "epoch": 0.023251874267150748,
      "grad_norm": 0.13975130021572113,
      "learning_rate": 4.999933433650744e-05,
      "loss": 0.0691,
      "step": 7000
    },
    {
      "epoch": 0.024912722429090088,
      "grad_norm": 0.18458342552185059,
      "learning_rate": 4.999923574399016e-05,
      "loss": 0.0676,
      "step": 7500
    },
    {
      "epoch": 0.026573570591029428,
      "grad_norm": 0.21281524002552032,
      "learning_rate": 4.9999130345560897e-05,
      "loss": 0.0672,
      "step": 8000
    },
    {
      "epoch": 0.028234418752968764,
      "grad_norm": 0.1605224311351776,
      "learning_rate": 4.999901814124836e-05,
      "loss": 0.0669,
      "step": 8500
    },
    {
      "epoch": 0.029895266914908104,
      "grad_norm": 0.1776961386203766,
      "learning_rate": 4.9998899131083076e-05,
      "loss": 0.0669,
      "step": 9000
    },
    {
      "epoch": 0.03155611507684745,
      "grad_norm": 0.3219272494316101,
      "learning_rate": 4.999877331509746e-05,
      "loss": 0.0657,
      "step": 9500
    },
    {
      "epoch": 0.033216963238786784,
      "grad_norm": 0.10419326275587082,
      "learning_rate": 4.999864069332576e-05,
      "loss": 0.0655,
      "step": 10000
    },
    {
      "epoch": 0.03487781140072612,
      "grad_norm": 0.13720539212226868,
      "learning_rate": 4.999850126580409e-05,
      "loss": 0.0653,
      "step": 10500
    },
    {
      "epoch": 0.036538659562665464,
      "grad_norm": 0.09529179334640503,
      "learning_rate": 4.99983550325704e-05,
      "loss": 0.0652,
      "step": 11000
    },
    {
      "epoch": 0.0381995077246048,
      "grad_norm": 0.2962712049484253,
      "learning_rate": 4.9998201993664496e-05,
      "loss": 0.065,
      "step": 11500
    },
    {
      "epoch": 0.039860355886544144,
      "grad_norm": 0.10641331225633621,
      "learning_rate": 4.999804214912806e-05,
      "loss": 0.0655,
      "step": 12000
    },
    {
      "epoch": 0.04152120404848348,
      "grad_norm": 0.1022527664899826,
      "learning_rate": 4.999787549900459e-05,
      "loss": 0.0632,
      "step": 12500
    },
    {
      "epoch": 0.04318205221042282,
      "grad_norm": 0.15500596165657043,
      "learning_rate": 4.999770204333945e-05,
      "loss": 0.0632,
      "step": 13000
    },
    {
      "epoch": 0.04484290037236216,
      "grad_norm": 0.09478642046451569,
      "learning_rate": 4.999752178217989e-05,
      "loss": 0.0628,
      "step": 13500
    },
    {
      "epoch": 0.046503748534301496,
      "grad_norm": 0.06178996339440346,
      "learning_rate": 4.9997334715574975e-05,
      "loss": 0.0642,
      "step": 14000
    },
    {
      "epoch": 0.04816459669624083,
      "grad_norm": 0.09243098646402359,
      "learning_rate": 4.9997140843575615e-05,
      "loss": 0.063,
      "step": 14500
    },
    {
      "epoch": 0.049825444858180176,
      "grad_norm": 0.12291423976421356,
      "learning_rate": 4.999694016623461e-05,
      "loss": 0.0626,
      "step": 15000
    },
    {
      "epoch": 0.05148629302011951,
      "grad_norm": 0.12098349630832672,
      "learning_rate": 4.999673268360659e-05,
      "loss": 0.0622,
      "step": 15500
    },
    {
      "epoch": 0.053147141182058856,
      "grad_norm": 0.13674767315387726,
      "learning_rate": 4.999651839574804e-05,
      "loss": 0.0604,
      "step": 16000
    },
    {
      "epoch": 0.05480798934399819,
      "grad_norm": 0.12830306589603424,
      "learning_rate": 4.999629730271729e-05,
      "loss": 0.0614,
      "step": 16500
    },
    {
      "epoch": 0.05646883750593753,
      "grad_norm": 0.12468775361776352,
      "learning_rate": 4.999606940457454e-05,
      "loss": 0.0619,
      "step": 17000
    },
    {
      "epoch": 0.05812968566787687,
      "grad_norm": 0.11636239290237427,
      "learning_rate": 4.999583470138184e-05,
      "loss": 0.063,
      "step": 17500
    },
    {
      "epoch": 0.05979053382981621,
      "grad_norm": 0.1791805773973465,
      "learning_rate": 4.999559319320307e-05,
      "loss": 0.0607,
      "step": 18000
    },
    {
      "epoch": 0.06145138199175555,
      "grad_norm": 0.07907412946224213,
      "learning_rate": 4.999534488010399e-05,
      "loss": 0.0605,
      "step": 18500
    },
    {
      "epoch": 0.0631122301536949,
      "grad_norm": 0.1044151708483696,
      "learning_rate": 4.999508976215221e-05,
      "loss": 0.0608,
      "step": 19000
    },
    {
      "epoch": 0.06477307831563422,
      "grad_norm": 0.19447346031665802,
      "learning_rate": 4.999482783941717e-05,
      "loss": 0.0608,
      "step": 19500
    },
    {
      "epoch": 0.06643392647757357,
      "grad_norm": 0.11067639291286469,
      "learning_rate": 4.999455911197018e-05,
      "loss": 0.0617,
      "step": 20000
    },
    {
      "epoch": 0.06809477463951291,
      "grad_norm": 0.16184329986572266,
      "learning_rate": 4.999428413773956e-05,
      "loss": 0.0618,
      "step": 20500
    },
    {
      "epoch": 0.06975562280145224,
      "grad_norm": 0.19073320925235748,
      "learning_rate": 4.9994001814699056e-05,
      "loss": 0.0601,
      "step": 21000
    },
    {
      "epoch": 0.07141647096339158,
      "grad_norm": 0.191606804728508,
      "learning_rate": 4.9993713272217366e-05,
      "loss": 0.0609,
      "step": 21500
    },
    {
      "epoch": 0.07307731912533093,
      "grad_norm": 0.15068641304969788,
      "learning_rate": 4.9993417353890184e-05,
      "loss": 0.0622,
      "step": 22000
    },
    {
      "epoch": 0.07473816728727026,
      "grad_norm": 0.09186579287052155,
      "learning_rate": 4.999311463123505e-05,
      "loss": 0.0601,
      "step": 22500
    },
    {
      "epoch": 0.0763990154492096,
      "grad_norm": 0.11882944405078888,
      "learning_rate": 4.999280510433438e-05,
      "loss": 0.0612,
      "step": 23000
    },
    {
      "epoch": 0.07805986361114894,
      "grad_norm": 0.10449936240911484,
      "learning_rate": 4.9992488773272444e-05,
      "loss": 0.0591,
      "step": 23500
    },
    {
      "epoch": 0.07972071177308829,
      "grad_norm": 0.10896172374486923,
      "learning_rate": 4.999216563813536e-05,
      "loss": 0.0606,
      "step": 24000
    },
    {
      "epoch": 0.08138155993502762,
      "grad_norm": 0.15048451721668243,
      "learning_rate": 4.9991835699011094e-05,
      "loss": 0.0591,
      "step": 24500
    },
    {
      "epoch": 0.08304240809696696,
      "grad_norm": 0.1036267876625061,
      "learning_rate": 4.999149895598948e-05,
      "loss": 0.0595,
      "step": 25000
    },
    {
      "epoch": 0.0847032562589063,
      "grad_norm": 0.22968466579914093,
      "learning_rate": 4.999115610304598e-05,
      "loss": 0.0584,
      "step": 25500
    },
    {
      "epoch": 0.08636410442084563,
      "grad_norm": 0.18597346544265747,
      "learning_rate": 4.999080576611386e-05,
      "loss": 0.0592,
      "step": 26000
    },
    {
      "epoch": 0.08802495258278498,
      "grad_norm": 0.18512657284736633,
      "learning_rate": 4.999044862556479e-05,
      "loss": 0.0592,
      "step": 26500
    },
    {
      "epoch": 0.08968580074472432,
      "grad_norm": 0.09914737194776535,
      "learning_rate": 4.9990084681495995e-05,
      "loss": 0.0605,
      "step": 27000
    },
    {
      "epoch": 0.09134664890666365,
      "grad_norm": 0.10745043307542801,
      "learning_rate": 4.998971468229127e-05,
      "loss": 0.0592,
      "step": 27500
    },
    {
      "epoch": 0.09300749706860299,
      "grad_norm": 0.12149012088775635,
      "learning_rate": 4.998933714508865e-05,
      "loss": 0.0584,
      "step": 28000
    },
    {
      "epoch": 0.09466834523054234,
      "grad_norm": 0.0972946435213089,
      "learning_rate": 4.998895280466889e-05,
      "loss": 0.0586,
      "step": 28500
    },
    {
      "epoch": 0.09632919339248167,
      "grad_norm": 0.1671963483095169,
      "learning_rate": 4.9988561661136637e-05,
      "loss": 0.0589,
      "step": 29000
    },
    {
      "epoch": 0.09799004155442101,
      "grad_norm": 0.1863366812467575,
      "learning_rate": 4.9988163714598366e-05,
      "loss": 0.0589,
      "step": 29500
    },
    {
      "epoch": 0.09965088971636035,
      "grad_norm": 0.12101081758737564,
      "learning_rate": 4.998775896516242e-05,
      "loss": 0.0594,
      "step": 30000
    },
    {
      "epoch": 0.1013117378782997,
      "grad_norm": 0.12294183671474457,
      "learning_rate": 4.998734824283254e-05,
      "loss": 0.0581,
      "step": 30500
    },
    {
      "epoch": 0.10297258604023903,
      "grad_norm": 0.11733397841453552,
      "learning_rate": 4.9986929901538914e-05,
      "loss": 0.0576,
      "step": 31000
    },
    {
      "epoch": 0.10463343420217837,
      "grad_norm": 0.16814886033535004,
      "learning_rate": 4.9986504757683505e-05,
      "loss": 0.0586,
      "step": 31500
    },
    {
      "epoch": 0.10629428236411771,
      "grad_norm": 0.09178508818149567,
      "learning_rate": 4.9986072811382065e-05,
      "loss": 0.0575,
      "step": 32000
    },
    {
      "epoch": 0.10795513052605704,
      "grad_norm": 0.07893312722444534,
      "learning_rate": 4.9985634062752196e-05,
      "loss": 0.0585,
      "step": 32500
    },
    {
      "epoch": 0.10961597868799638,
      "grad_norm": 0.1641961634159088,
      "learning_rate": 4.998518940980353e-05,
      "loss": 0.0578,
      "step": 33000
    },
    {
      "epoch": 0.11127682684993573,
      "grad_norm": 0.09987607598304749,
      "learning_rate": 4.998473707048103e-05,
      "loss": 0.0582,
      "step": 33500
    },
    {
      "epoch": 0.11293767501187506,
      "grad_norm": 0.1254776120185852,
      "learning_rate": 4.9984277929193746e-05,
      "loss": 0.0583,
      "step": 34000
    },
    {
      "epoch": 0.1145985231738144,
      "grad_norm": 0.09947583079338074,
      "learning_rate": 4.998381198606667e-05,
      "loss": 0.0588,
      "step": 34500
    },
    {
      "epoch": 0.11625937133575374,
      "grad_norm": 0.19605961441993713,
      "learning_rate": 4.998334019350435e-05,
      "loss": 0.0575,
      "step": 35000
    },
    {
      "epoch": 0.11792021949769309,
      "grad_norm": 0.09208977222442627,
      "learning_rate": 4.9982860660683133e-05,
      "loss": 0.0588,
      "step": 35500
    },
    {
      "epoch": 0.11958106765963242,
      "grad_norm": 0.1156262531876564,
      "learning_rate": 4.998237432640797e-05,
      "loss": 0.0569,
      "step": 36000
    },
    {
      "epoch": 0.12124191582157176,
      "grad_norm": 0.13122783601284027,
      "learning_rate": 4.9981881190811266e-05,
      "loss": 0.0587,
      "step": 36500
    },
    {
      "epoch": 0.1229027639835111,
      "grad_norm": 0.13994242250919342,
      "learning_rate": 4.9981382260688335e-05,
      "loss": 0.0581,
      "step": 37000
    },
    {
      "epoch": 0.12456361214545043,
      "grad_norm": 0.07988868653774261,
      "learning_rate": 4.9980875536455116e-05,
      "loss": 0.058,
      "step": 37500
    },
    {
      "epoch": 0.1262244603073898,
      "grad_norm": 0.0974101647734642,
      "learning_rate": 4.99803620113084e-05,
      "loss": 0.0588,
      "step": 38000
    },
    {
      "epoch": 0.1278853084693291,
      "grad_norm": 0.08321130275726318,
      "learning_rate": 4.997984168538798e-05,
      "loss": 0.057,
      "step": 38500
    },
    {
      "epoch": 0.12954615663126845,
      "grad_norm": 0.12150345742702484,
      "learning_rate": 4.997931455883552e-05,
      "loss": 0.0569,
      "step": 39000
    },
    {
      "epoch": 0.1312070047932078,
      "grad_norm": 0.1506723165512085,
      "learning_rate": 4.997878170643538e-05,
      "loss": 0.0574,
      "step": 39500
    },
    {
      "epoch": 0.13286785295514714,
      "grad_norm": 0.1010110154747963,
      "learning_rate": 4.9978240992651746e-05,
      "loss": 0.0564,
      "step": 40000
    },
    {
      "epoch": 0.13452870111708648,
      "grad_norm": 0.1439797729253769,
      "learning_rate": 4.997769347867184e-05,
      "loss": 0.0566,
      "step": 40500
    },
    {
      "epoch": 0.13618954927902582,
      "grad_norm": 0.0659145787358284,
      "learning_rate": 4.9977139164644726e-05,
      "loss": 0.0583,
      "step": 41000
    },
    {
      "epoch": 0.13785039744096514,
      "grad_norm": 0.1089615523815155,
      "learning_rate": 4.9976578050721314e-05,
      "loss": 0.057,
      "step": 41500
    },
    {
      "epoch": 0.13951124560290448,
      "grad_norm": 0.10898468643426895,
      "learning_rate": 4.997601127966774e-05,
      "loss": 0.0557,
      "step": 42000
    },
    {
      "epoch": 0.14117209376484383,
      "grad_norm": 0.06977932155132294,
      "learning_rate": 4.997543658001089e-05,
      "loss": 0.0577,
      "step": 42500
    },
    {
      "epoch": 0.14283294192678317,
      "grad_norm": 0.15164615213871002,
      "learning_rate": 4.997485508092126e-05,
      "loss": 0.0552,
      "step": 43000
    },
    {
      "epoch": 0.1444937900887225,
      "grad_norm": 0.11123768985271454,
      "learning_rate": 4.997426678255715e-05,
      "loss": 0.0583,
      "step": 43500
    },
    {
      "epoch": 0.14615463825066186,
      "grad_norm": 0.12780867516994476,
      "learning_rate": 4.997367288205912e-05,
      "loss": 0.0565,
      "step": 44000
    },
    {
      "epoch": 0.1478154864126012,
      "grad_norm": 0.12140152603387833,
      "learning_rate": 4.9973070999226156e-05,
      "loss": 0.0564,
      "step": 44500
    },
    {
      "epoch": 0.14947633457454051,
      "grad_norm": 0.08238749951124191,
      "learning_rate": 4.997246231760444e-05,
      "loss": 0.0557,
      "step": 45000
    },
    {
      "epoch": 0.15113718273647986,
      "grad_norm": 0.09342442452907562,
      "learning_rate": 4.997184683735967e-05,
      "loss": 0.0561,
      "step": 45500
    },
    {
      "epoch": 0.1527980308984192,
      "grad_norm": 0.2013527750968933,
      "learning_rate": 4.9971224558659414e-05,
      "loss": 0.057,
      "step": 46000
    },
    {
      "epoch": 0.15445887906035854,
      "grad_norm": 0.12124834209680557,
      "learning_rate": 4.997059674661162e-05,
      "loss": 0.058,
      "step": 46500
    },
    {
      "epoch": 0.1561197272222979,
      "grad_norm": 0.12933015823364258,
      "learning_rate": 4.996996088510654e-05,
      "loss": 0.0556,
      "step": 47000
    },
    {
      "epoch": 0.15778057538423723,
      "grad_norm": 0.0638279914855957,
      "learning_rate": 4.996931822565939e-05,
      "loss": 0.0561,
      "step": 47500
    },
    {
      "epoch": 0.15944142354617657,
      "grad_norm": 0.09399716556072235,
      "learning_rate": 4.9968668768445157e-05,
      "loss": 0.0562,
      "step": 48000
    },
    {
      "epoch": 0.1611022717081159,
      "grad_norm": 0.14391371607780457,
      "learning_rate": 4.9968012513640663e-05,
      "loss": 0.0559,
      "step": 48500
    },
    {
      "epoch": 0.16276311987005523,
      "grad_norm": 0.06520704925060272,
      "learning_rate": 4.996734946142454e-05,
      "loss": 0.0565,
      "step": 49000
    },
    {
      "epoch": 0.16442396803199458,
      "grad_norm": 0.11551161110401154,
      "learning_rate": 4.996668095845972e-05,
      "loss": 0.0572,
      "step": 49500
    },
    {
      "epoch": 0.16608481619393392,
      "grad_norm": 0.23073971271514893,
      "learning_rate": 4.996600432555768e-05,
      "loss": 0.0557,
      "step": 50000
    },
    {
      "epoch": 0.16774566435587326,
      "grad_norm": 0.09237940609455109,
      "learning_rate": 4.9965320895790734e-05,
      "loss": 0.0572,
      "step": 50500
    },
    {
      "epoch": 0.1694065125178126,
      "grad_norm": 0.09792722016572952,
      "learning_rate": 4.996463066934496e-05,
      "loss": 0.0563,
      "step": 51000
    },
    {
      "epoch": 0.17106736067975192,
      "grad_norm": 0.08041338622570038,
      "learning_rate": 4.9963933646408255e-05,
      "loss": 0.056,
      "step": 51500
    },
    {
      "epoch": 0.17272820884169127,
      "grad_norm": 0.09318117797374725,
      "learning_rate": 4.9963229827170386e-05,
      "loss": 0.056,
      "step": 52000
    },
    {
      "epoch": 0.1743890570036306,
      "grad_norm": 0.15545104444026947,
      "learning_rate": 4.9962519211822964e-05,
      "loss": 0.0561,
      "step": 52500
    },
    {
      "epoch": 0.17604990516556995,
      "grad_norm": 0.10292993485927582,
      "learning_rate": 4.996180180055946e-05,
      "loss": 0.0565,
      "step": 53000
    },
    {
      "epoch": 0.1777107533275093,
      "grad_norm": 0.24959829449653625,
      "learning_rate": 4.996107904877113e-05,
      "loss": 0.0538,
      "step": 53500
    },
    {
      "epoch": 0.17937160148944864,
      "grad_norm": 0.1946813315153122,
      "learning_rate": 4.9960349528613703e-05,
      "loss": 0.0564,
      "step": 54000
    },
    {
      "epoch": 0.18103244965138798,
      "grad_norm": 0.155516117811203,
      "learning_rate": 4.9959611757962085e-05,
      "loss": 0.0555,
      "step": 54500
    },
    {
      "epoch": 0.1826932978133273,
      "grad_norm": 0.08121763914823532,
      "learning_rate": 4.995886719218591e-05,
      "loss": 0.0555,
      "step": 55000
    },
    {
      "epoch": 0.18435414597526664,
      "grad_norm": 0.10283124446868896,
      "learning_rate": 4.995811583148789e-05,
      "loss": 0.0554,
      "step": 55500
    },
    {
      "epoch": 0.18601499413720599,
      "grad_norm": 0.13081420958042145,
      "learning_rate": 4.99573591991644e-05,
      "loss": 0.0551,
      "step": 56000
    },
    {
      "epoch": 0.18767584229914533,
      "grad_norm": 0.07537112385034561,
      "learning_rate": 4.995659426282701e-05,
      "loss": 0.0547,
      "step": 56500
    },
    {
      "epoch": 0.18933669046108467,
      "grad_norm": 0.2632521986961365,
      "learning_rate": 4.9955822532186566e-05,
      "loss": 0.055,
      "step": 57000
    },
    {
      "epoch": 0.19099753862302402,
      "grad_norm": 0.10522643476724625,
      "learning_rate": 4.9955044007453164e-05,
      "loss": 0.0551,
      "step": 57500
    },
    {
      "epoch": 0.19265838678496333,
      "grad_norm": 0.09127593040466309,
      "learning_rate": 4.995425868883876e-05,
      "loss": 0.0563,
      "step": 58000
    },
    {
      "epoch": 0.19431923494690267,
      "grad_norm": 0.14918848872184753,
      "learning_rate": 4.9953468167561644e-05,
      "loss": 0.0538,
      "step": 58500
    },
    {
      "epoch": 0.19598008310884202,
      "grad_norm": 0.0914568305015564,
      "learning_rate": 4.9952669275415164e-05,
      "loss": 0.056,
      "step": 59000
    },
    {
      "epoch": 0.19764093127078136,
      "grad_norm": 0.13708922266960144,
      "learning_rate": 4.9951863590034183e-05,
      "loss": 0.0552,
      "step": 59500
    },
    {
      "epoch": 0.1993017794327207,
      "grad_norm": 0.08608783036470413,
      "learning_rate": 4.9951051111638044e-05,
      "loss": 0.0553,
      "step": 60000
    },
    {
      "epoch": 0.20096262759466005,
      "grad_norm": 0.13105562329292297,
      "learning_rate": 4.995023184044795e-05,
      "loss": 0.0543,
      "step": 60500
    },
    {
      "epoch": 0.2026234757565994,
      "grad_norm": 0.14140138030052185,
      "learning_rate": 4.994940577668693e-05,
      "loss": 0.0545,
      "step": 61000
    },
    {
      "epoch": 0.2042843239185387,
      "grad_norm": 0.19245865941047668,
      "learning_rate": 4.9948572920579885e-05,
      "loss": 0.0548,
      "step": 61500
    },
    {
      "epoch": 0.20594517208047805,
      "grad_norm": 0.17885583639144897,
      "learning_rate": 4.994773327235355e-05,
      "loss": 0.0538,
      "step": 62000
    },
    {
      "epoch": 0.2076060202424174,
      "grad_norm": 0.10892090201377869,
      "learning_rate": 4.994688853189492e-05,
      "loss": 0.0545,
      "step": 62500
    },
    {
      "epoch": 0.20926686840435674,
      "grad_norm": 0.16388724744319916,
      "learning_rate": 4.994603531370072e-05,
      "loss": 0.0557,
      "step": 63000
    },
    {
      "epoch": 0.21092771656629608,
      "grad_norm": 0.09224208444356918,
      "learning_rate": 4.994517530407808e-05,
      "loss": 0.0549,
      "step": 63500
    },
    {
      "epoch": 0.21258856472823542,
      "grad_norm": 0.23569777607917786,
      "learning_rate": 4.9944308503261154e-05,
      "loss": 0.0558,
      "step": 64000
    },
    {
      "epoch": 0.21424941289017477,
      "grad_norm": 0.16281235218048096,
      "learning_rate": 4.9943436665446666e-05,
      "loss": 0.0549,
      "step": 64500
    },
    {
      "epoch": 0.21591026105211408,
      "grad_norm": 0.1524936556816101,
      "learning_rate": 4.9942556296532125e-05,
      "loss": 0.0547,
      "step": 65000
    },
    {
      "epoch": 0.21757110921405343,
      "grad_norm": 0.09440302103757858,
      "learning_rate": 4.994166913713629e-05,
      "loss": 0.0546,
      "step": 65500
    },
    {
      "epoch": 0.21923195737599277,
      "grad_norm": 0.18124116957187653,
      "learning_rate": 4.994077518750069e-05,
      "loss": 0.055,
      "step": 66000
    },
    {
      "epoch": 0.2208928055379321,
      "grad_norm": 0.16271071135997772,
      "learning_rate": 4.993987444786869e-05,
      "loss": 0.0544,
      "step": 66500
    },
    {
      "epoch": 0.22255365369987146,
      "grad_norm": 0.20797821879386902,
      "learning_rate": 4.993896691848553e-05,
      "loss": 0.0543,
      "step": 67000
    },
    {
      "epoch": 0.2242145018618108,
      "grad_norm": 0.2142292708158493,
      "learning_rate": 4.993805259959825e-05,
      "loss": 0.0551,
      "step": 67500
    },
    {
      "epoch": 0.22587535002375012,
      "grad_norm": 0.06313934922218323,
      "learning_rate": 4.99371333404476e-05,
      "loss": 0.0537,
      "step": 68000
    },
    {
      "epoch": 0.22753619818568946,
      "grad_norm": 0.10533276945352554,
      "learning_rate": 4.9936205456878484e-05,
      "loss": 0.0538,
      "step": 68500
    },
    {
      "epoch": 0.2291970463476288,
      "grad_norm": 0.1080014780163765,
      "learning_rate": 4.993527078455706e-05,
      "loss": 0.055,
      "step": 69000
    },
    {
      "epoch": 0.23085789450956815,
      "grad_norm": 0.11372676491737366,
      "learning_rate": 4.993432932373778e-05,
      "loss": 0.0546,
      "step": 69500
    },
    {
      "epoch": 0.2325187426715075,
      "grad_norm": 0.14183025062084198,
      "learning_rate": 4.993338107467698e-05,
      "loss": 0.0553,
      "step": 70000
    },
    {
      "epoch": 0.23417959083344683,
      "grad_norm": 0.06318558007478714,
      "learning_rate": 4.9932426037632774e-05,
      "loss": 0.0542,
      "step": 70500
    },
    {
      "epoch": 0.23584043899538618,
      "grad_norm": 0.11108233034610748,
      "learning_rate": 4.993146614328871e-05,
      "loss": 0.0545,
      "step": 71000
    },
    {
      "epoch": 0.2375012871573255,
      "grad_norm": 0.10301029682159424,
      "learning_rate": 4.993049754463426e-05,
      "loss": 0.0541,
      "step": 71500
    },
    {
      "epoch": 0.23916213531926483,
      "grad_norm": 0.08198980242013931,
      "learning_rate": 4.992952215878145e-05,
      "loss": 0.0538,
      "step": 72000
    },
    {
      "epoch": 0.24082298348120418,
      "grad_norm": 0.07906068116426468,
      "learning_rate": 4.9928539985995826e-05,
      "loss": 0.0552,
      "step": 72500
    },
    {
      "epoch": 0.24248383164314352,
      "grad_norm": 0.17845383286476135,
      "learning_rate": 4.9927553011236594e-05,
      "loss": 0.0554,
      "step": 73000
    },
    {
      "epoch": 0.24414467980508286,
      "grad_norm": 0.07445243000984192,
      "learning_rate": 4.9926557278961885e-05,
      "loss": 0.0553,
      "step": 73500
    },
    {
      "epoch": 0.2458055279670222,
      "grad_norm": 0.10080838948488235,
      "learning_rate": 4.992555476056153e-05,
      "loss": 0.0529,
      "step": 74000
    },
    {
      "epoch": 0.24746637612896155,
      "grad_norm": 0.12892822921276093,
      "learning_rate": 4.992454545630847e-05,
      "loss": 0.0533,
      "step": 74500
    },
    {
      "epoch": 0.24912722429090087,
      "grad_norm": 0.10400581359863281,
      "learning_rate": 4.9923531405428955e-05,
      "loss": 0.0543,
      "step": 75000
    },
    {
      "epoch": 0.25078807245284024,
      "grad_norm": 0.11286815255880356,
      "learning_rate": 4.992250854386697e-05,
      "loss": 0.0533,
      "step": 75500
    },
    {
      "epoch": 0.2524489206147796,
      "grad_norm": 0.07622205466032028,
      "learning_rate": 4.992147889728159e-05,
      "loss": 0.0539,
      "step": 76000
    },
    {
      "epoch": 0.25410976877671887,
      "grad_norm": 0.10283403843641281,
      "learning_rate": 4.9920442465953135e-05,
      "loss": 0.0537,
      "step": 76500
    },
    {
      "epoch": 0.2557706169386582,
      "grad_norm": 0.08055205643177032,
      "learning_rate": 4.991939925016376e-05,
      "loss": 0.0544,
      "step": 77000
    },
    {
      "epoch": 0.25743146510059756,
      "grad_norm": 0.0790947824716568,
      "learning_rate": 4.991834925019748e-05,
      "loss": 0.0534,
      "step": 77500
    },
    {
      "epoch": 0.2590923132625369,
      "grad_norm": 0.10500911623239517,
      "learning_rate": 4.991729246634015e-05,
      "loss": 0.0531,
      "step": 78000
    },
    {
      "epoch": 0.26075316142447624,
      "grad_norm": 0.08845368027687073,
      "learning_rate": 4.991622889887948e-05,
      "loss": 0.055,
      "step": 78500
    },
    {
      "epoch": 0.2624140095864156,
      "grad_norm": 0.14601926505565643,
      "learning_rate": 4.9915160695576114e-05,
      "loss": 0.0548,
      "step": 79000
    },
    {
      "epoch": 0.26407485774835493,
      "grad_norm": 0.13540877401828766,
      "learning_rate": 4.9914083575345e-05,
      "loss": 0.0541,
      "step": 79500
    },
    {
      "epoch": 0.2657357059102943,
      "grad_norm": 0.09290409088134766,
      "learning_rate": 4.9912999672384144e-05,
      "loss": 0.0541,
      "step": 80000
    },
    {
      "epoch": 0.2673965540722336,
      "grad_norm": 0.10025584697723389,
      "learning_rate": 4.99119111751281e-05,
      "loss": 0.0537,
      "step": 80500
    },
    {
      "epoch": 0.26905740223417296,
      "grad_norm": 0.1885160505771637,
      "learning_rate": 4.991081372115884e-05,
      "loss": 0.0538,
      "step": 81000
    },
    {
      "epoch": 0.2707182503961123,
      "grad_norm": 0.1143856942653656,
      "learning_rate": 4.9909709485350054e-05,
      "loss": 0.0536,
      "step": 81500
    },
    {
      "epoch": 0.27237909855805165,
      "grad_norm": 0.1657995581626892,
      "learning_rate": 4.9908598468002346e-05,
      "loss": 0.0536,
      "step": 82000
    },
    {
      "epoch": 0.274039946719991,
      "grad_norm": 0.10317113250494003,
      "learning_rate": 4.990748066941818e-05,
      "loss": 0.054,
      "step": 82500
    },
    {
      "epoch": 0.2757007948819303,
      "grad_norm": 0.0822257399559021,
      "learning_rate": 4.990635834582809e-05,
      "loss": 0.0541,
      "step": 83000
    },
    {
      "epoch": 0.2773616430438696,
      "grad_norm": 0.07550226151943207,
      "learning_rate": 4.990522699924676e-05,
      "loss": 0.0534,
      "step": 83500
    },
    {
      "epoch": 0.27902249120580896,
      "grad_norm": 0.10179945826530457,
      "learning_rate": 4.9904088872346855e-05,
      "loss": 0.0537,
      "step": 84000
    },
    {
      "epoch": 0.2806833393677483,
      "grad_norm": 0.13194836676120758,
      "learning_rate": 4.990294396543821e-05,
      "loss": 0.0533,
      "step": 84500
    },
    {
      "epoch": 0.28234418752968765,
      "grad_norm": 0.10745211690664291,
      "learning_rate": 4.990179227883253e-05,
      "loss": 0.0536,
      "step": 85000
    },
    {
      "epoch": 0.284005035691627,
      "grad_norm": 0.09491731226444244,
      "learning_rate": 4.990063381284335e-05,
      "loss": 0.0532,
      "step": 85500
    },
    {
      "epoch": 0.28566588385356634,
      "grad_norm": 0.11661045253276825,
      "learning_rate": 4.9899468567786044e-05,
      "loss": 0.0535,
      "step": 86000
    },
    {
      "epoch": 0.2873267320155057,
      "grad_norm": 0.17740832269191742,
      "learning_rate": 4.9898296543977866e-05,
      "loss": 0.0542,
      "step": 86500
    },
    {
      "epoch": 0.288987580177445,
      "grad_norm": 0.10659551620483398,
      "learning_rate": 4.989712010610702e-05,
      "loss": 0.0538,
      "step": 87000
    },
    {
      "epoch": 0.29064842833938437,
      "grad_norm": 0.1237768828868866,
      "learning_rate": 4.989593453931206e-05,
      "loss": 0.053,
      "step": 87500
    },
    {
      "epoch": 0.2923092765013237,
      "grad_norm": 0.1719592660665512,
      "learning_rate": 4.989474219472833e-05,
      "loss": 0.0534,
      "step": 88000
    },
    {
      "epoch": 0.29397012466326305,
      "grad_norm": 0.050181616097688675,
      "learning_rate": 4.989354307268046e-05,
      "loss": 0.0536,
      "step": 88500
    },
    {
      "epoch": 0.2956309728252024,
      "grad_norm": 0.14107686281204224,
      "learning_rate": 4.989233959205662e-05,
      "loss": 0.0522,
      "step": 89000
    },
    {
      "epoch": 0.2972918209871417,
      "grad_norm": 0.10008332878351212,
      "learning_rate": 4.989112692961494e-05,
      "loss": 0.0534,
      "step": 89500
    },
    {
      "epoch": 0.29895266914908103,
      "grad_norm": 0.1339002102613449,
      "learning_rate": 4.9889907490693346e-05,
      "loss": 0.0523,
      "step": 90000
    },
    {
      "epoch": 0.3006135173110204,
      "grad_norm": 0.08209329843521118,
      "learning_rate": 4.988868373481634e-05,
      "loss": 0.0538,
      "step": 90500
    },
    {
      "epoch": 0.3022743654729597,
      "grad_norm": 0.17832693457603455,
      "learning_rate": 4.988745075748401e-05,
      "loss": 0.0536,
      "step": 91000
    },
    {
      "epoch": 0.30393521363489906,
      "grad_norm": 0.09133152663707733,
      "learning_rate": 4.9886211004672586e-05,
      "loss": 0.0539,
      "step": 91500
    },
    {
      "epoch": 0.3055960617968384,
      "grad_norm": 0.12430620193481445,
      "learning_rate": 4.988496447671959e-05,
      "loss": 0.0532,
      "step": 92000
    },
    {
      "epoch": 0.30725690995877775,
      "grad_norm": 0.0882280021905899,
      "learning_rate": 4.9883711173964374e-05,
      "loss": 0.053,
      "step": 92500
    },
    {
      "epoch": 0.3089177581207171,
      "grad_norm": 0.10489823669195175,
      "learning_rate": 4.9882451096748145e-05,
      "loss": 0.0526,
      "step": 93000
    },
    {
      "epoch": 0.31057860628265643,
      "grad_norm": 0.11505074054002762,
      "learning_rate": 4.988118424541395e-05,
      "loss": 0.0534,
      "step": 93500
    },
    {
      "epoch": 0.3122394544445958,
      "grad_norm": 0.10375227779150009,
      "learning_rate": 4.987991062030669e-05,
      "loss": 0.0529,
      "step": 94000
    },
    {
      "epoch": 0.3139003026065351,
      "grad_norm": 0.12749110162258148,
      "learning_rate": 4.9878632789329805e-05,
      "loss": 0.0538,
      "step": 94500
    },
    {
      "epoch": 0.31556115076847446,
      "grad_norm": 0.505192220211029,
      "learning_rate": 4.987734563126426e-05,
      "loss": 0.0525,
      "step": 95000
    },
    {
      "epoch": 0.3172219989304138,
      "grad_norm": 0.1515541970729828,
      "learning_rate": 4.9876051700470695e-05,
      "loss": 0.0526,
      "step": 95500
    },
    {
      "epoch": 0.31888284709235315,
      "grad_norm": 0.17634665966033936,
      "learning_rate": 4.9874750997301365e-05,
      "loss": 0.0533,
      "step": 96000
    },
    {
      "epoch": 0.32054369525429244,
      "grad_norm": 0.10289124399423599,
      "learning_rate": 4.9873443522110384e-05,
      "loss": 0.0529,
      "step": 96500
    },
    {
      "epoch": 0.3222045434162318,
      "grad_norm": 0.12020827829837799,
      "learning_rate": 4.987212927525371e-05,
      "loss": 0.0534,
      "step": 97000
    },
    {
      "epoch": 0.3238653915781711,
      "grad_norm": 0.060654621571302414,
      "learning_rate": 4.9870810905882985e-05,
      "loss": 0.0528,
      "step": 97500
    },
    {
      "epoch": 0.32552623974011047,
      "grad_norm": 0.2092723399400711,
      "learning_rate": 4.9869483130311676e-05,
      "loss": 0.0539,
      "step": 98000
    },
    {
      "epoch": 0.3271870879020498,
      "grad_norm": 0.08161595463752747,
      "learning_rate": 4.986814858415287e-05,
      "loss": 0.0534,
      "step": 98500
    },
    {
      "epoch": 0.32884793606398915,
      "grad_norm": 0.156625434756279,
      "learning_rate": 4.986680726776989e-05,
      "loss": 0.0512,
      "step": 99000
    },
    {
      "epoch": 0.3305087842259285,
      "grad_norm": 0.17626701295375824,
      "learning_rate": 4.9865459181527895e-05,
      "loss": 0.0535,
      "step": 99500
    },
    {
      "epoch": 0.33216963238786784,
      "grad_norm": 0.15620069205760956,
      "learning_rate": 4.986410432579389e-05,
      "loss": 0.0534,
      "step": 100000
    },
    {
      "epoch": 0.3338304805498072,
      "grad_norm": 0.06931280344724655,
      "learning_rate": 4.986274270093674e-05,
      "loss": 0.0525,
      "step": 100500
    },
    {
      "epoch": 0.33549132871174653,
      "grad_norm": 0.10119476169347763,
      "learning_rate": 4.9861374307327134e-05,
      "loss": 0.0531,
      "step": 101000
    },
    {
      "epoch": 0.33715217687368587,
      "grad_norm": 0.13499611616134644,
      "learning_rate": 4.9860001902416186e-05,
      "loss": 0.0533,
      "step": 101500
    },
    {
      "epoch": 0.3388130250356252,
      "grad_norm": 0.08095595985651016,
      "learning_rate": 4.985861998595676e-05,
      "loss": 0.0527,
      "step": 102000
    },
    {
      "epoch": 0.34047387319756456,
      "grad_norm": 0.1149311512708664,
      "learning_rate": 4.985723130186727e-05,
      "loss": 0.0521,
      "step": 102500
    },
    {
      "epoch": 0.34213472135950385,
      "grad_norm": 0.14246594905853271,
      "learning_rate": 4.985583585052577e-05,
      "loss": 0.0535,
      "step": 103000
    },
    {
      "epoch": 0.3437955695214432,
      "grad_norm": 0.08528705686330795,
      "learning_rate": 4.985443644350169e-05,
      "loss": 0.0527,
      "step": 103500
    },
    {
      "epoch": 0.34545641768338253,
      "grad_norm": 0.1313210129737854,
      "learning_rate": 4.985302747233033e-05,
      "loss": 0.0527,
      "step": 104000
    },
    {
      "epoch": 0.3471172658453219,
      "grad_norm": 0.1005023717880249,
      "learning_rate": 4.985161173505144e-05,
      "loss": 0.0534,
      "step": 104500
    },
    {
      "epoch": 0.3487781140072612,
      "grad_norm": 0.2814842760562897,
      "learning_rate": 4.9850189232050445e-05,
      "loss": 0.0536,
      "step": 105000
    },
    {
      "epoch": 0.35043896216920056,
      "grad_norm": 0.07776834070682526,
      "learning_rate": 4.984876282900283e-05,
      "loss": 0.0529,
      "step": 105500
    },
    {
      "epoch": 0.3520998103311399,
      "grad_norm": 0.10589730739593506,
      "learning_rate": 4.9847329688041426e-05,
      "loss": 0.0535,
      "step": 106000
    },
    {
      "epoch": 0.35376065849307925,
      "grad_norm": 0.09382622689008713,
      "learning_rate": 4.984588691726253e-05,
      "loss": 0.0517,
      "step": 106500
    },
    {
      "epoch": 0.3554215066550186,
      "grad_norm": 0.08032596111297607,
      "learning_rate": 4.984443738232007e-05,
      "loss": 0.0522,
      "step": 107000
    },
    {
      "epoch": 0.35708235481695794,
      "grad_norm": 0.13799209892749786,
      "learning_rate": 4.984298108360869e-05,
      "loss": 0.0531,
      "step": 107500
    },
    {
      "epoch": 0.3587432029788973,
      "grad_norm": 0.08617257326841354,
      "learning_rate": 4.9841518021524845e-05,
      "loss": 0.0532,
      "step": 108000
    },
    {
      "epoch": 0.3604040511408366,
      "grad_norm": 0.09660615772008896,
      "learning_rate": 4.984004819646686e-05,
      "loss": 0.0523,
      "step": 108500
    },
    {
      "epoch": 0.36206489930277597,
      "grad_norm": 0.15800511837005615,
      "learning_rate": 4.983857160883488e-05,
      "loss": 0.0532,
      "step": 109000
    },
    {
      "epoch": 0.36372574746471525,
      "grad_norm": 0.13293606042861938,
      "learning_rate": 4.983708825903089e-05,
      "loss": 0.0517,
      "step": 109500
    },
    {
      "epoch": 0.3653865956266546,
      "grad_norm": 0.11097883433103561,
      "learning_rate": 4.9835601134429865e-05,
      "loss": 0.0523,
      "step": 110000
    },
    {
      "epoch": 0.36704744378859394,
      "grad_norm": 0.09935025125741959,
      "learning_rate": 4.983410427501754e-05,
      "loss": 0.0527,
      "step": 110500
    },
    {
      "epoch": 0.3687082919505333,
      "grad_norm": 0.08198618143796921,
      "learning_rate": 4.983260366863732e-05,
      "loss": 0.051,
      "step": 111000
    },
    {
      "epoch": 0.37036914011247263,
      "grad_norm": 0.16744659841060638,
      "learning_rate": 4.9831093301243445e-05,
      "loss": 0.0522,
      "step": 111500
    },
    {
      "epoch": 0.37202998827441197,
      "grad_norm": 0.13476820290088654,
      "learning_rate": 4.982957617371351e-05,
      "loss": 0.0528,
      "step": 112000
    },
    {
      "epoch": 0.3736908364363513,
      "grad_norm": 0.05297614634037018,
      "learning_rate": 4.9828052286460526e-05,
      "loss": 0.0524,
      "step": 112500
    },
    {
      "epoch": 0.37535168459829066,
      "grad_norm": 0.08622632175683975,
      "learning_rate": 4.9826521639899374e-05,
      "loss": 0.0522,
      "step": 113000
    },
    {
      "epoch": 0.37701253276023,
      "grad_norm": 0.07616276293992996,
      "learning_rate": 4.982498423444676e-05,
      "loss": 0.0521,
      "step": 113500
    },
    {
      "epoch": 0.37867338092216934,
      "grad_norm": 0.10231181979179382,
      "learning_rate": 4.982344007052123e-05,
      "loss": 0.0528,
      "step": 114000
    },
    {
      "epoch": 0.3803342290841087,
      "grad_norm": 0.0996473953127861,
      "learning_rate": 4.9821889148543194e-05,
      "loss": 0.0523,
      "step": 114500
    },
    {
      "epoch": 0.38199507724604803,
      "grad_norm": 0.2104177474975586,
      "learning_rate": 4.9820334591037914e-05,
      "loss": 0.0528,
      "step": 115000
    },
    {
      "epoch": 0.3836559254079874,
      "grad_norm": 0.07004857808351517,
      "learning_rate": 4.9818770167737355e-05,
      "loss": 0.0522,
      "step": 115500
    },
    {
      "epoch": 0.38531677356992666,
      "grad_norm": 0.10454750061035156,
      "learning_rate": 4.981719898765563e-05,
      "loss": 0.0522,
      "step": 116000
    },
    {
      "epoch": 0.386977621731866,
      "grad_norm": 0.09745889902114868,
      "learning_rate": 4.9815624213835924e-05,
      "loss": 0.0529,
      "step": 116500
    },
    {
      "epoch": 0.38863846989380535,
      "grad_norm": 0.08930981904268265,
      "learning_rate": 4.9814039534988376e-05,
      "loss": 0.0522,
      "step": 117000
    },
    {
      "epoch": 0.3902993180557447,
      "grad_norm": 0.11502919346094131,
      "learning_rate": 4.9812448100647554e-05,
      "loss": 0.0516,
      "step": 117500
    },
    {
      "epoch": 0.39196016621768404,
      "grad_norm": 0.10479524731636047,
      "learning_rate": 4.981084991124671e-05,
      "loss": 0.0521,
      "step": 118000
    },
    {
      "epoch": 0.3936210143796234,
      "grad_norm": 0.0594681017100811,
      "learning_rate": 4.980924496722097e-05,
      "loss": 0.052,
      "step": 118500
    },
    {
      "epoch": 0.3952818625415627,
      "grad_norm": 0.17416034638881683,
      "learning_rate": 4.9807633269007246e-05,
      "loss": 0.0519,
      "step": 119000
    },
    {
      "epoch": 0.39694271070350207,
      "grad_norm": 0.06075098738074303,
      "learning_rate": 4.980601481704433e-05,
      "loss": 0.0526,
      "step": 119500
    },
    {
      "epoch": 0.3986035588654414,
      "grad_norm": 0.21688245236873627,
      "learning_rate": 4.980438961177283e-05,
      "loss": 0.0511,
      "step": 120000
    },
    {
      "epoch": 0.40026440702738075,
      "grad_norm": 0.12255538254976273,
      "learning_rate": 4.980276092429053e-05,
      "loss": 0.052,
      "step": 120500
    },
    {
      "epoch": 0.4019252551893201,
      "grad_norm": 0.07818970084190369,
      "learning_rate": 4.980112222723546e-05,
      "loss": 0.0532,
      "step": 121000
    },
    {
      "epoch": 0.40358610335125944,
      "grad_norm": 0.1106751561164856,
      "learning_rate": 4.979947677820378e-05,
      "loss": 0.0522,
      "step": 121500
    },
    {
      "epoch": 0.4052469515131988,
      "grad_norm": 0.1469191610813141,
      "learning_rate": 4.979782457764347e-05,
      "loss": 0.0516,
      "step": 122000
    },
    {
      "epoch": 0.4069077996751381,
      "grad_norm": 0.14643041789531708,
      "learning_rate": 4.979616562600433e-05,
      "loss": 0.0523,
      "step": 122500
    },
    {
      "epoch": 0.4085686478370774,
      "grad_norm": 0.15677116811275482,
      "learning_rate": 4.9794499923737984e-05,
      "loss": 0.0521,
      "step": 123000
    },
    {
      "epoch": 0.41022949599901676,
      "grad_norm": 0.08887089788913727,
      "learning_rate": 4.979282747129793e-05,
      "loss": 0.0516,
      "step": 123500
    },
    {
      "epoch": 0.4118903441609561,
      "grad_norm": 0.09876196831464767,
      "learning_rate": 4.9791148269139484e-05,
      "loss": 0.0518,
      "step": 124000
    },
    {
      "epoch": 0.41355119232289544,
      "grad_norm": 0.06985071301460266,
      "learning_rate": 4.978946569635809e-05,
      "loss": 0.0516,
      "step": 124500
    },
    {
      "epoch": 0.4152120404848348,
      "grad_norm": 0.0974465012550354,
      "learning_rate": 4.978777300963329e-05,
      "loss": 0.0511,
      "step": 125000
    },
    {
      "epoch": 0.41687288864677413,
      "grad_norm": 0.06277813762426376,
      "learning_rate": 4.9786073574566145e-05,
      "loss": 0.0521,
      "step": 125500
    },
    {
      "epoch": 0.4185337368087135,
      "grad_norm": 0.10529668629169464,
      "learning_rate": 4.9784367391619314e-05,
      "loss": 0.0518,
      "step": 126000
    },
    {
      "epoch": 0.4201945849706528,
      "grad_norm": 0.11008229851722717,
      "learning_rate": 4.978266132641898e-05,
      "loss": 0.0509,
      "step": 126500
    },
    {
      "epoch": 0.42185543313259216,
      "grad_norm": 0.06576377153396606,
      "learning_rate": 4.9780941676094985e-05,
      "loss": 0.053,
      "step": 127000
    },
    {
      "epoch": 0.4235162812945315,
      "grad_norm": 0.1450151801109314,
      "learning_rate": 4.977921527928843e-05,
      "loss": 0.0512,
      "step": 127500
    },
    {
      "epoch": 0.42517712945647085,
      "grad_norm": 0.05652409791946411,
      "learning_rate": 4.9777482136469334e-05,
      "loss": 0.0518,
      "step": 128000
    },
    {
      "epoch": 0.4268379776184102,
      "grad_norm": 0.10272032022476196,
      "learning_rate": 4.977574224810952e-05,
      "loss": 0.0502,
      "step": 128500
    },
    {
      "epoch": 0.42849882578034953,
      "grad_norm": 0.12655287981033325,
      "learning_rate": 4.9773995614682677e-05,
      "loss": 0.0519,
      "step": 129000
    },
    {
      "epoch": 0.4301596739422888,
      "grad_norm": 0.08489731699228287,
      "learning_rate": 4.9772245750151135e-05,
      "loss": 0.0516,
      "step": 129500
    },
    {
      "epoch": 0.43182052210422817,
      "grad_norm": 0.09050401300191879,
      "learning_rate": 4.9770485641506345e-05,
      "loss": 0.053,
      "step": 130000
    },
    {
      "epoch": 0.4334813702661675,
      "grad_norm": 0.06386271119117737,
      "learning_rate": 4.9768718789225604e-05,
      "loss": 0.0522,
      "step": 130500
    },
    {
      "epoch": 0.43514221842810685,
      "grad_norm": 0.0697174146771431,
      "learning_rate": 4.9766945193789926e-05,
      "loss": 0.0514,
      "step": 131000
    },
    {
      "epoch": 0.4368030665900462,
      "grad_norm": 0.10007945448160172,
      "learning_rate": 4.9765168423087246e-05,
      "loss": 0.0513,
      "step": 131500
    },
    {
      "epoch": 0.43846391475198554,
      "grad_norm": 0.09958118200302124,
      "learning_rate": 4.9763381356275996e-05,
      "loss": 0.051,
      "step": 132000
    },
    {
      "epoch": 0.4401247629139249,
      "grad_norm": 0.10016599297523499,
      "learning_rate": 4.976158754776289e-05,
      "loss": 0.0515,
      "step": 132500
    },
    {
      "epoch": 0.4417856110758642,
      "grad_norm": 0.0858168751001358,
      "learning_rate": 4.975978699803629e-05,
      "loss": 0.0504,
      "step": 133000
    },
    {
      "epoch": 0.44344645923780357,
      "grad_norm": 0.11982491612434387,
      "learning_rate": 4.97579797075864e-05,
      "loss": 0.0508,
      "step": 133500
    },
    {
      "epoch": 0.4451073073997429,
      "grad_norm": 0.11241217702627182,
      "learning_rate": 4.9756169311693005e-05,
      "loss": 0.0518,
      "step": 134000
    },
    {
      "epoch": 0.44676815556168226,
      "grad_norm": 0.13209277391433716,
      "learning_rate": 4.97543485547534e-05,
      "loss": 0.0513,
      "step": 134500
    },
    {
      "epoch": 0.4484290037236216,
      "grad_norm": 0.08754147589206696,
      "learning_rate": 4.975252105857108e-05,
      "loss": 0.0517,
      "step": 135000
    },
    {
      "epoch": 0.45008985188556094,
      "grad_norm": 0.07228536158800125,
      "learning_rate": 4.9750686823643575e-05,
      "loss": 0.0532,
      "step": 135500
    },
    {
      "epoch": 0.45175070004750023,
      "grad_norm": 0.11034371703863144,
      "learning_rate": 4.974884585047025e-05,
      "loss": 0.0511,
      "step": 136000
    },
    {
      "epoch": 0.4534115482094396,
      "grad_norm": 0.08611790090799332,
      "learning_rate": 4.974700184169806e-05,
      "loss": 0.052,
      "step": 136500
    },
    {
      "epoch": 0.4550723963713789,
      "grad_norm": 0.19146889448165894,
      "learning_rate": 4.9745147407012485e-05,
      "loss": 0.0515,
      "step": 137000
    },
    {
      "epoch": 0.45673324453331826,
      "grad_norm": 0.0674821063876152,
      "learning_rate": 4.974328623558915e-05,
      "loss": 0.0532,
      "step": 137500
    },
    {
      "epoch": 0.4583940926952576,
      "grad_norm": 0.11000882834196091,
      "learning_rate": 4.9741418327934776e-05,
      "loss": 0.0519,
      "step": 138000
    },
    {
      "epoch": 0.46005494085719695,
      "grad_norm": 0.08358696848154068,
      "learning_rate": 4.9739543684557864e-05,
      "loss": 0.0515,
      "step": 138500
    },
    {
      "epoch": 0.4617157890191363,
      "grad_norm": 0.168899804353714,
      "learning_rate": 4.97376623059688e-05,
      "loss": 0.0518,
      "step": 139000
    },
    {
      "epoch": 0.46337663718107563,
      "grad_norm": 0.05821722373366356,
      "learning_rate": 4.973577797562723e-05,
      "loss": 0.0521,
      "step": 139500
    },
    {
      "epoch": 0.465037485343015,
      "grad_norm": 0.13056249916553497,
      "learning_rate": 4.973388314162011e-05,
      "loss": 0.0506,
      "step": 140000
    },
    {
      "epoch": 0.4666983335049543,
      "grad_norm": 0.1359417736530304,
      "learning_rate": 4.973198157394188e-05,
      "loss": 0.0511,
      "step": 140500
    },
    {
      "epoch": 0.46835918166689366,
      "grad_norm": 0.11205544322729111,
      "learning_rate": 4.9730073273110235e-05,
      "loss": 0.0515,
      "step": 141000
    },
    {
      "epoch": 0.470020029828833,
      "grad_norm": 0.1259387731552124,
      "learning_rate": 4.972815823964469e-05,
      "loss": 0.0521,
      "step": 141500
    },
    {
      "epoch": 0.47168087799077235,
      "grad_norm": 0.13825741410255432,
      "learning_rate": 4.972623647406661e-05,
      "loss": 0.0505,
      "step": 142000
    },
    {
      "epoch": 0.47334172615271164,
      "grad_norm": 0.1054653599858284,
      "learning_rate": 4.972430797689919e-05,
      "loss": 0.0513,
      "step": 142500
    },
    {
      "epoch": 0.475002574314651,
      "grad_norm": 0.052442390471696854,
      "learning_rate": 4.972237274866745e-05,
      "loss": 0.0501,
      "step": 143000
    },
    {
      "epoch": 0.4766634224765903,
      "grad_norm": 0.1147526279091835,
      "learning_rate": 4.9720434680532506e-05,
      "loss": 0.0506,
      "step": 143500
    },
    {
      "epoch": 0.47832427063852967,
      "grad_norm": 0.10074964910745621,
      "learning_rate": 4.971848600521401e-05,
      "loss": 0.0515,
      "step": 144000
    },
    {
      "epoch": 0.479985118800469,
      "grad_norm": 0.10736691951751709,
      "learning_rate": 4.97165306004162e-05,
      "loss": 0.0509,
      "step": 144500
    },
    {
      "epoch": 0.48164596696240836,
      "grad_norm": 0.06613104790449142,
      "learning_rate": 4.9714572397654034e-05,
      "loss": 0.0514,
      "step": 145000
    },
    {
      "epoch": 0.4833068151243477,
      "grad_norm": 0.13994215428829193,
      "learning_rate": 4.971260354895275e-05,
      "loss": 0.0508,
      "step": 145500
    },
    {
      "epoch": 0.48496766328628704,
      "grad_norm": 0.1678117960691452,
      "learning_rate": 4.9710627972373615e-05,
      "loss": 0.0512,
      "step": 146000
    },
    {
      "epoch": 0.4866285114482264,
      "grad_norm": 0.04353756085038185,
      "learning_rate": 4.9708645668454465e-05,
      "loss": 0.051,
      "step": 146500
    },
    {
      "epoch": 0.48828935961016573,
      "grad_norm": 0.07851290702819824,
      "learning_rate": 4.97066606225094e-05,
      "loss": 0.0501,
      "step": 147000
    },
    {
      "epoch": 0.4899502077721051,
      "grad_norm": 0.10224834084510803,
      "learning_rate": 4.970466487898305e-05,
      "loss": 0.0511,
      "step": 147500
    },
    {
      "epoch": 0.4916110559340444,
      "grad_norm": 0.16390590369701385,
      "learning_rate": 4.970266240974012e-05,
      "loss": 0.0522,
      "step": 148000
    },
    {
      "epoch": 0.49327190409598376,
      "grad_norm": 0.07085056602954865,
      "learning_rate": 4.970065321532575e-05,
      "loss": 0.0507,
      "step": 148500
    },
    {
      "epoch": 0.4949327522579231,
      "grad_norm": 0.14370110630989075,
      "learning_rate": 4.9698637296286955e-05,
      "loss": 0.0523,
      "step": 149000
    },
    {
      "epoch": 0.4965936004198624,
      "grad_norm": 0.07595096528530121,
      "learning_rate": 4.9696614653172536e-05,
      "loss": 0.0515,
      "step": 149500
    },
    {
      "epoch": 0.49825444858180173,
      "grad_norm": 0.06164243817329407,
      "learning_rate": 4.969458528653317e-05,
      "loss": 0.051,
      "step": 150000
    },
    {
      "epoch": 0.4999152967437411,
      "grad_norm": 0.08353562653064728,
      "learning_rate": 4.969254919692132e-05,
      "loss": 0.0525,
      "step": 150500
    },
    {
      "epoch": 0.5015761449056805,
      "grad_norm": 0.0709356889128685,
      "learning_rate": 4.969051047722398e-05,
      "loss": 0.0511,
      "step": 151000
    },
    {
      "epoch": 0.5032369930676198,
      "grad_norm": 0.08291640877723694,
      "learning_rate": 4.9688465062524084e-05,
      "loss": 0.0511,
      "step": 151500
    },
    {
      "epoch": 0.5048978412295592,
      "grad_norm": 0.08316313475370407,
      "learning_rate": 4.968640883421213e-05,
      "loss": 0.0516,
      "step": 152000
    },
    {
      "epoch": 0.5065586893914984,
      "grad_norm": 0.07912985980510712,
      "learning_rate": 4.9684345885153703e-05,
      "loss": 0.0515,
      "step": 152500
    },
    {
      "epoch": 0.5082195375534377,
      "grad_norm": 0.13744328916072845,
      "learning_rate": 4.968227621591043e-05,
      "loss": 0.0514,
      "step": 153000
    },
    {
      "epoch": 0.5098803857153771,
      "grad_norm": 0.14089231193065643,
      "learning_rate": 4.968019982704575e-05,
      "loss": 0.0508,
      "step": 153500
    },
    {
      "epoch": 0.5115412338773164,
      "grad_norm": 0.07048705965280533,
      "learning_rate": 4.967811671912498e-05,
      "loss": 0.0506,
      "step": 154000
    },
    {
      "epoch": 0.5132020820392558,
      "grad_norm": 0.12405050545930862,
      "learning_rate": 4.9676026892715204e-05,
      "loss": 0.0523,
      "step": 154500
    },
    {
      "epoch": 0.5148629302011951,
      "grad_norm": 0.17090043425559998,
      "learning_rate": 4.967393034838539e-05,
      "loss": 0.0521,
      "step": 155000
    },
    {
      "epoch": 0.5165237783631345,
      "grad_norm": 0.10644062608480453,
      "learning_rate": 4.96718270867063e-05,
      "loss": 0.0509,
      "step": 155500
    },
    {
      "epoch": 0.5181846265250738,
      "grad_norm": 0.0846187025308609,
      "learning_rate": 4.966972556154342e-05,
      "loss": 0.0519,
      "step": 156000
    },
    {
      "epoch": 0.5198454746870131,
      "grad_norm": 0.12063046544790268,
      "learning_rate": 4.966760889374907e-05,
      "loss": 0.0517,
      "step": 156500
    },
    {
      "epoch": 0.5215063228489525,
      "grad_norm": 0.09144708514213562,
      "learning_rate": 4.966548551032644e-05,
      "loss": 0.0502,
      "step": 157000
    },
    {
      "epoch": 0.5231671710108918,
      "grad_norm": 0.08404906094074249,
      "learning_rate": 4.96633554118536e-05,
      "loss": 0.0516,
      "step": 157500
    },
    {
      "epoch": 0.5248280191728312,
      "grad_norm": 0.13476219773292542,
      "learning_rate": 4.9661218598910455e-05,
      "loss": 0.0506,
      "step": 158000
    },
    {
      "epoch": 0.5264888673347705,
      "grad_norm": 0.09794363379478455,
      "learning_rate": 4.965907507207874e-05,
      "loss": 0.0507,
      "step": 158500
    },
    {
      "epoch": 0.5281497154967099,
      "grad_norm": 0.07096827030181885,
      "learning_rate": 4.965692483194203e-05,
      "loss": 0.0521,
      "step": 159000
    },
    {
      "epoch": 0.5298105636586492,
      "grad_norm": 0.07045737653970718,
      "learning_rate": 4.9654767879085705e-05,
      "loss": 0.0512,
      "step": 159500
    },
    {
      "epoch": 0.5314714118205885,
      "grad_norm": 0.1417117863893509,
      "learning_rate": 4.965260854812529e-05,
      "loss": 0.0499,
      "step": 160000
    },
    {
      "epoch": 0.5331322599825279,
      "grad_norm": 0.181583970785141,
      "learning_rate": 4.9650442532439676e-05,
      "loss": 0.052,
      "step": 160500
    },
    {
      "epoch": 0.5347931081444672,
      "grad_norm": 0.1214839369058609,
      "learning_rate": 4.964826547179778e-05,
      "loss": 0.0513,
      "step": 161000
    },
    {
      "epoch": 0.5364539563064066,
      "grad_norm": 0.06555894017219543,
      "learning_rate": 4.964608170079373e-05,
      "loss": 0.0509,
      "step": 161500
    },
    {
      "epoch": 0.5381148044683459,
      "grad_norm": 0.13326352834701538,
      "learning_rate": 4.964389122002206e-05,
      "loss": 0.0504,
      "step": 162000
    },
    {
      "epoch": 0.5397756526302853,
      "grad_norm": 0.10696954280138016,
      "learning_rate": 4.964169403007912e-05,
      "loss": 0.0513,
      "step": 162500
    },
    {
      "epoch": 0.5414365007922246,
      "grad_norm": 0.06590669602155685,
      "learning_rate": 4.963949013156307e-05,
      "loss": 0.0507,
      "step": 163000
    },
    {
      "epoch": 0.543097348954164,
      "grad_norm": 0.04486642777919769,
      "learning_rate": 4.9637279525073924e-05,
      "loss": 0.0499,
      "step": 163500
    },
    {
      "epoch": 0.5447581971161033,
      "grad_norm": 0.09004158526659012,
      "learning_rate": 4.963506665253477e-05,
      "loss": 0.0506,
      "step": 164000
    },
    {
      "epoch": 0.5464190452780426,
      "grad_norm": 0.08244810253381729,
      "learning_rate": 4.963284710002703e-05,
      "loss": 0.0501,
      "step": 164500
    },
    {
      "epoch": 0.548079893439982,
      "grad_norm": 0.07833255082368851,
      "learning_rate": 4.963061640006026e-05,
      "loss": 0.0513,
      "step": 165000
    },
    {
      "epoch": 0.5497407416019213,
      "grad_norm": 0.1296864002943039,
      "learning_rate": 4.962837899453622e-05,
      "loss": 0.0519,
      "step": 165500
    },
    {
      "epoch": 0.5514015897638606,
      "grad_norm": 0.10222948342561722,
      "learning_rate": 4.9626134884064024e-05,
      "loss": 0.0516,
      "step": 166000
    },
    {
      "epoch": 0.5530624379257999,
      "grad_norm": 0.0929526686668396,
      "learning_rate": 4.962388406925462e-05,
      "loss": 0.0495,
      "step": 166500
    },
    {
      "epoch": 0.5547232860877392,
      "grad_norm": 0.1354484260082245,
      "learning_rate": 4.962162655072079e-05,
      "loss": 0.0517,
      "step": 167000
    },
    {
      "epoch": 0.5563841342496786,
      "grad_norm": 0.11013169586658478,
      "learning_rate": 4.9619362329077115e-05,
      "loss": 0.0499,
      "step": 167500
    },
    {
      "epoch": 0.5580449824116179,
      "grad_norm": 0.08973590284585953,
      "learning_rate": 4.961709140494003e-05,
      "loss": 0.0514,
      "step": 168000
    },
    {
      "epoch": 0.5597058305735573,
      "grad_norm": 0.11766836792230606,
      "learning_rate": 4.961481834086786e-05,
      "loss": 0.051,
      "step": 168500
    },
    {
      "epoch": 0.5613666787354966,
      "grad_norm": 0.09905312955379486,
      "learning_rate": 4.96125340270024e-05,
      "loss": 0.0504,
      "step": 169000
    },
    {
      "epoch": 0.563027526897436,
      "grad_norm": 0.09273596853017807,
      "learning_rate": 4.96102430125025e-05,
      "loss": 0.0507,
      "step": 169500
    },
    {
      "epoch": 0.5646883750593753,
      "grad_norm": 0.09203031659126282,
      "learning_rate": 4.960794529799186e-05,
      "loss": 0.0505,
      "step": 170000
    },
    {
      "epoch": 0.5663492232213146,
      "grad_norm": 0.07157156616449356,
      "learning_rate": 4.960564088409605e-05,
      "loss": 0.0509,
      "step": 170500
    },
    {
      "epoch": 0.568010071383254,
      "grad_norm": 0.0823153406381607,
      "learning_rate": 4.960332977144241e-05,
      "loss": 0.0507,
      "step": 171000
    },
    {
      "epoch": 0.5696709195451933,
      "grad_norm": 0.0920790433883667,
      "learning_rate": 4.960101660296601e-05,
      "loss": 0.051,
      "step": 171500
    },
    {
      "epoch": 0.5713317677071327,
      "grad_norm": 0.19907346367835999,
      "learning_rate": 4.959869210808048e-05,
      "loss": 0.0514,
      "step": 172000
    },
    {
      "epoch": 0.572992615869072,
      "grad_norm": 0.07074791938066483,
      "learning_rate": 4.95963609163289e-05,
      "loss": 0.0505,
      "step": 172500
    },
    {
      "epoch": 0.5746534640310114,
      "grad_norm": 0.09227843582630157,
      "learning_rate": 4.959402302834591e-05,
      "loss": 0.0511,
      "step": 173000
    },
    {
      "epoch": 0.5763143121929507,
      "grad_norm": 0.12039509415626526,
      "learning_rate": 4.959167844476801e-05,
      "loss": 0.0512,
      "step": 173500
    },
    {
      "epoch": 0.57797516035489,
      "grad_norm": 0.09587520360946655,
      "learning_rate": 4.958932716623348e-05,
      "loss": 0.0502,
      "step": 174000
    },
    {
      "epoch": 0.5796360085168294,
      "grad_norm": 0.13247881829738617,
      "learning_rate": 4.958696919338246e-05,
      "loss": 0.0497,
      "step": 174500
    },
    {
      "epoch": 0.5812968566787687,
      "grad_norm": 0.1250840574502945,
      "learning_rate": 4.958460452685688e-05,
      "loss": 0.0496,
      "step": 175000
    },
    {
      "epoch": 0.5829577048407081,
      "grad_norm": 0.09441531449556351,
      "learning_rate": 4.9582233167300525e-05,
      "loss": 0.0506,
      "step": 175500
    },
    {
      "epoch": 0.5846185530026474,
      "grad_norm": 0.13265131413936615,
      "learning_rate": 4.957985987814143e-05,
      "loss": 0.0517,
      "step": 176000
    },
    {
      "epoch": 0.5862794011645868,
      "grad_norm": 0.10940689593553543,
      "learning_rate": 4.957747514784492e-05,
      "loss": 0.0499,
      "step": 176500
    },
    {
      "epoch": 0.5879402493265261,
      "grad_norm": 0.07154802232980728,
      "learning_rate": 4.957508372645857e-05,
      "loss": 0.0504,
      "step": 177000
    },
    {
      "epoch": 0.5896010974884655,
      "grad_norm": 0.18749888241291046,
      "learning_rate": 4.957268561463343e-05,
      "loss": 0.0515,
      "step": 177500
    },
    {
      "epoch": 0.5912619456504048,
      "grad_norm": 0.08108267933130264,
      "learning_rate": 4.957028562930156e-05,
      "loss": 0.0502,
      "step": 178000
    },
    {
      "epoch": 0.5929227938123441,
      "grad_norm": 0.09619064629077911,
      "learning_rate": 4.9567874151936866e-05,
      "loss": 0.0514,
      "step": 178500
    },
    {
      "epoch": 0.5945836419742834,
      "grad_norm": 0.09472031891345978,
      "learning_rate": 4.9565455986096155e-05,
      "loss": 0.0508,
      "step": 179000
    },
    {
      "epoch": 0.5962444901362227,
      "grad_norm": 0.10897838324308395,
      "learning_rate": 4.956303113243775e-05,
      "loss": 0.0502,
      "step": 179500
    },
    {
      "epoch": 0.5979053382981621,
      "grad_norm": 0.10068939626216888,
      "learning_rate": 4.956060446137679e-05,
      "loss": 0.0518,
      "step": 180000
    },
    {
      "epoch": 0.5995661864601014,
      "grad_norm": 0.12798139452934265,
      "learning_rate": 4.955816624743761e-05,
      "loss": 0.051,
      "step": 180500
    },
    {
      "epoch": 0.6012270346220407,
      "grad_norm": 0.10571622103452682,
      "learning_rate": 4.955572134766533e-05,
      "loss": 0.0515,
      "step": 181000
    },
    {
      "epoch": 0.6028878827839801,
      "grad_norm": 0.05757640674710274,
      "learning_rate": 4.955327467256681e-05,
      "loss": 0.0509,
      "step": 181500
    },
    {
      "epoch": 0.6045487309459194,
      "grad_norm": 0.15275149047374725,
      "learning_rate": 4.955081641649531e-05,
      "loss": 0.0497,
      "step": 182000
    },
    {
      "epoch": 0.6062095791078588,
      "grad_norm": 0.08724714070558548,
      "learning_rate": 4.954835147659168e-05,
      "loss": 0.0516,
      "step": 182500
    },
    {
      "epoch": 0.6078704272697981,
      "grad_norm": 0.06655170023441315,
      "learning_rate": 4.9545879853526965e-05,
      "loss": 0.0488,
      "step": 183000
    },
    {
      "epoch": 0.6095312754317375,
      "grad_norm": 0.09494264423847198,
      "learning_rate": 4.954340154797407e-05,
      "loss": 0.0501,
      "step": 183500
    },
    {
      "epoch": 0.6111921235936768,
      "grad_norm": 0.0915137380361557,
      "learning_rate": 4.954092153725041e-05,
      "loss": 0.0503,
      "step": 184000
    },
    {
      "epoch": 0.6128529717556161,
      "grad_norm": 0.10637243837118149,
      "learning_rate": 4.953842988210865e-05,
      "loss": 0.0503,
      "step": 184500
    },
    {
      "epoch": 0.6145138199175555,
      "grad_norm": 0.0691116452217102,
      "learning_rate": 4.953593154650692e-05,
      "loss": 0.0502,
      "step": 185000
    },
    {
      "epoch": 0.6161746680794948,
      "grad_norm": 0.08647464960813522,
      "learning_rate": 4.953342653112537e-05,
      "loss": 0.0502,
      "step": 185500
    },
    {
      "epoch": 0.6178355162414342,
      "grad_norm": 0.06521870195865631,
      "learning_rate": 4.953091483664598e-05,
      "loss": 0.0513,
      "step": 186000
    },
    {
      "epoch": 0.6194963644033735,
      "grad_norm": 0.20431753993034363,
      "learning_rate": 4.9528401507162937e-05,
      "loss": 0.0514,
      "step": 186500
    },
    {
      "epoch": 0.6211572125653129,
      "grad_norm": 0.12451162934303284,
      "learning_rate": 4.952587646989585e-05,
      "loss": 0.0512,
      "step": 187000
    },
    {
      "epoch": 0.6228180607272522,
      "grad_norm": 0.0840567946434021,
      "learning_rate": 4.952334475558638e-05,
      "loss": 0.0506,
      "step": 187500
    },
    {
      "epoch": 0.6244789088891916,
      "grad_norm": 0.07184825837612152,
      "learning_rate": 4.952080636492379e-05,
      "loss": 0.0512,
      "step": 188000
    },
    {
      "epoch": 0.6261397570511309,
      "grad_norm": 0.0627768486738205,
      "learning_rate": 4.951826129859912e-05,
      "loss": 0.0509,
      "step": 188500
    },
    {
      "epoch": 0.6278006052130702,
      "grad_norm": 0.05265979468822479,
      "learning_rate": 4.9515709557305276e-05,
      "loss": 0.0507,
      "step": 189000
    },
    {
      "epoch": 0.6294614533750096,
      "grad_norm": 0.12073932588100433,
      "learning_rate": 4.951315114173695e-05,
      "loss": 0.05,
      "step": 189500
    },
    {
      "epoch": 0.6311223015369489,
      "grad_norm": 0.11011894047260284,
      "learning_rate": 4.95105911894287e-05,
      "loss": 0.052,
      "step": 190000
    },
    {
      "epoch": 0.6327831496988883,
      "grad_norm": 0.12633849680423737,
      "learning_rate": 4.950801944074783e-05,
      "loss": 0.0515,
      "step": 190500
    },
    {
      "epoch": 0.6344439978608276,
      "grad_norm": 0.10826751589775085,
      "learning_rate": 4.950544101988606e-05,
      "loss": 0.0503,
      "step": 191000
    },
    {
      "epoch": 0.636104846022767,
      "grad_norm": 0.05760490149259567,
      "learning_rate": 4.9502855927545354e-05,
      "loss": 0.0508,
      "step": 191500
    },
    {
      "epoch": 0.6377656941847063,
      "grad_norm": 0.05529234558343887,
      "learning_rate": 4.95002641644295e-05,
      "loss": 0.0509,
      "step": 192000
    },
    {
      "epoch": 0.6394265423466455,
      "grad_norm": 0.09603679925203323,
      "learning_rate": 4.949766573124409e-05,
      "loss": 0.0503,
      "step": 192500
    },
    {
      "epoch": 0.6410873905085849,
      "grad_norm": 0.12162504345178604,
      "learning_rate": 4.9495060628696534e-05,
      "loss": 0.0493,
      "step": 193000
    },
    {
      "epoch": 0.6427482386705242,
      "grad_norm": 0.11880820989608765,
      "learning_rate": 4.949244885749605e-05,
      "loss": 0.0503,
      "step": 193500
    },
    {
      "epoch": 0.6444090868324636,
      "grad_norm": 0.1803562045097351,
      "learning_rate": 4.948983566188611e-05,
      "loss": 0.0502,
      "step": 194000
    },
    {
      "epoch": 0.6460699349944029,
      "grad_norm": 0.1422390192747116,
      "learning_rate": 4.948721582568796e-05,
      "loss": 0.05,
      "step": 194500
    },
    {
      "epoch": 0.6477307831563422,
      "grad_norm": 0.09018667787313461,
      "learning_rate": 4.9484584079466856e-05,
      "loss": 0.0501,
      "step": 195000
    },
    {
      "epoch": 0.6493916313182816,
      "grad_norm": 0.11058380454778671,
      "learning_rate": 4.948194566744501e-05,
      "loss": 0.0507,
      "step": 195500
    },
    {
      "epoch": 0.6510524794802209,
      "grad_norm": 0.08759795129299164,
      "learning_rate": 4.9479300590340716e-05,
      "loss": 0.0504,
      "step": 196000
    },
    {
      "epoch": 0.6527133276421603,
      "grad_norm": 0.09925734996795654,
      "learning_rate": 4.9476648848874097e-05,
      "loss": 0.05,
      "step": 196500
    },
    {
      "epoch": 0.6543741758040996,
      "grad_norm": 0.078234001994133,
      "learning_rate": 4.9473990443767067e-05,
      "loss": 0.0483,
      "step": 197000
    },
    {
      "epoch": 0.656035023966039,
      "grad_norm": 0.1121828705072403,
      "learning_rate": 4.947132537574336e-05,
      "loss": 0.0501,
      "step": 197500
    },
    {
      "epoch": 0.6576958721279783,
      "grad_norm": 0.05234669893980026,
      "learning_rate": 4.9468653645528526e-05,
      "loss": 0.0513,
      "step": 198000
    },
    {
      "epoch": 0.6593567202899177,
      "grad_norm": 0.10224280506372452,
      "learning_rate": 4.946598598068532e-05,
      "loss": 0.0502,
      "step": 198500
    },
    {
      "epoch": 0.661017568451857,
      "grad_norm": 0.08427224308252335,
      "learning_rate": 4.9463300954913636e-05,
      "loss": 0.05,
      "step": 199000
    },
    {
      "epoch": 0.6626784166137963,
      "grad_norm": 0.09776391088962555,
      "learning_rate": 4.946060926913544e-05,
      "loss": 0.0504,
      "step": 199500
    },
    {
      "epoch": 0.6643392647757357,
      "grad_norm": 0.15521912276744843,
      "learning_rate": 4.9457910924083516e-05,
      "loss": 0.0499,
      "step": 200000
    },
    {
      "epoch": 0.666000112937675,
      "grad_norm": 0.04334206506609917,
      "learning_rate": 4.9455211337144406e-05,
      "loss": 0.0499,
      "step": 200500
    },
    {
      "epoch": 0.6676609610996144,
      "grad_norm": 0.06274409592151642,
      "learning_rate": 4.9452499689065557e-05,
      "loss": 0.0507,
      "step": 201000
    },
    {
      "epoch": 0.6693218092615537,
      "grad_norm": 0.08557017892599106,
      "learning_rate": 4.944978138392078e-05,
      "loss": 0.0496,
      "step": 201500
    },
    {
      "epoch": 0.6709826574234931,
      "grad_norm": 0.12350325286388397,
      "learning_rate": 4.944705642245012e-05,
      "loss": 0.0491,
      "step": 202000
    },
    {
      "epoch": 0.6726435055854324,
      "grad_norm": 0.11571907997131348,
      "learning_rate": 4.944432480539544e-05,
      "loss": 0.051,
      "step": 202500
    },
    {
      "epoch": 0.6743043537473717,
      "grad_norm": 0.15411071479320526,
      "learning_rate": 4.9441586533500394e-05,
      "loss": 0.0493,
      "step": 203000
    },
    {
      "epoch": 0.6759652019093111,
      "grad_norm": 0.08922380954027176,
      "learning_rate": 4.943884710400275e-05,
      "loss": 0.0498,
      "step": 203500
    },
    {
      "epoch": 0.6776260500712504,
      "grad_norm": 0.09778347611427307,
      "learning_rate": 4.9436095537971196e-05,
      "loss": 0.0505,
      "step": 204000
    },
    {
      "epoch": 0.6792868982331898,
      "grad_norm": 0.07427288591861725,
      "learning_rate": 4.943333731933965e-05,
      "loss": 0.0507,
      "step": 204500
    },
    {
      "epoch": 0.6809477463951291,
      "grad_norm": 0.08676030486822128,
      "learning_rate": 4.943057244885903e-05,
      "loss": 0.0493,
      "step": 205000
    },
    {
      "epoch": 0.6826085945570683,
      "grad_norm": 0.10290338844060898,
      "learning_rate": 4.9427800927282077e-05,
      "loss": 0.0518,
      "step": 205500
    },
    {
      "epoch": 0.6842694427190077,
      "grad_norm": 0.09584570676088333,
      "learning_rate": 4.942502275536329e-05,
      "loss": 0.0502,
      "step": 206000
    },
    {
      "epoch": 0.685930290880947,
      "grad_norm": 0.09896513819694519,
      "learning_rate": 4.942223793385903e-05,
      "loss": 0.0489,
      "step": 206500
    },
    {
      "epoch": 0.6875911390428864,
      "grad_norm": 0.15373645722866058,
      "learning_rate": 4.941944646352745e-05,
      "loss": 0.0497,
      "step": 207000
    },
    {
      "epoch": 0.6892519872048257,
      "grad_norm": 0.10236050933599472,
      "learning_rate": 4.941665394799958e-05,
      "loss": 0.0511,
      "step": 207500
    },
    {
      "epoch": 0.6909128353667651,
      "grad_norm": 0.1307530403137207,
      "learning_rate": 4.9413849195588905e-05,
      "loss": 0.0489,
      "step": 208000
    },
    {
      "epoch": 0.6925736835287044,
      "grad_norm": 0.08010069280862808,
      "learning_rate": 4.9411037796634697e-05,
      "loss": 0.0498,
      "step": 208500
    },
    {
      "epoch": 0.6942345316906438,
      "grad_norm": 0.06509663164615631,
      "learning_rate": 4.940821975190234e-05,
      "loss": 0.0502,
      "step": 209000
    },
    {
      "epoch": 0.6958953798525831,
      "grad_norm": 0.08165103197097778,
      "learning_rate": 4.9405400718169745e-05,
      "loss": 0.0495,
      "step": 209500
    },
    {
      "epoch": 0.6975562280145224,
      "grad_norm": 0.065231092274189,
      "learning_rate": 4.9402569397472225e-05,
      "loss": 0.0509,
      "step": 210000
    },
    {
      "epoch": 0.6992170761764618,
      "grad_norm": 0.09796100109815598,
      "learning_rate": 4.939973143330204e-05,
      "loss": 0.0493,
      "step": 210500
    },
    {
      "epoch": 0.7008779243384011,
      "grad_norm": 0.06226392462849617,
      "learning_rate": 4.939688682643181e-05,
      "loss": 0.0497,
      "step": 211000
    },
    {
      "epoch": 0.7025387725003405,
      "grad_norm": 0.07081454247236252,
      "learning_rate": 4.939404128676169e-05,
      "loss": 0.0498,
      "step": 211500
    },
    {
      "epoch": 0.7041996206622798,
      "grad_norm": 0.07894018292427063,
      "learning_rate": 4.9391183410097984e-05,
      "loss": 0.0501,
      "step": 212000
    },
    {
      "epoch": 0.7058604688242192,
      "grad_norm": 0.0961623564362526,
      "learning_rate": 4.93883188930614e-05,
      "loss": 0.0497,
      "step": 212500
    },
    {
      "epoch": 0.7075213169861585,
      "grad_norm": 0.07529482245445251,
      "learning_rate": 4.9385447736431774e-05,
      "loss": 0.0496,
      "step": 213000
    },
    {
      "epoch": 0.7091821651480978,
      "grad_norm": 0.12854768335819244,
      "learning_rate": 4.938257570320666e-05,
      "loss": 0.0489,
      "step": 213500
    },
    {
      "epoch": 0.7108430133100372,
      "grad_norm": 0.07598186284303665,
      "learning_rate": 4.9379691283013006e-05,
      "loss": 0.0504,
      "step": 214000
    },
    {
      "epoch": 0.7125038614719765,
      "grad_norm": 0.0921049565076828,
      "learning_rate": 4.937680022557513e-05,
      "loss": 0.0504,
      "step": 214500
    },
    {
      "epoch": 0.7141647096339159,
      "grad_norm": 0.1028127372264862,
      "learning_rate": 4.937390253168012e-05,
      "loss": 0.0514,
      "step": 215000
    },
    {
      "epoch": 0.7158255577958552,
      "grad_norm": 0.08944687247276306,
      "learning_rate": 4.9371004017397836e-05,
      "loss": 0.0495,
      "step": 215500
    },
    {
      "epoch": 0.7174864059577946,
      "grad_norm": 0.09744902700185776,
      "learning_rate": 4.9368093066225955e-05,
      "loss": 0.0503,
      "step": 216000
    },
    {
      "epoch": 0.7191472541197339,
      "grad_norm": 0.09814135730266571,
      "learning_rate": 4.936517548096741e-05,
      "loss": 0.0492,
      "step": 216500
    },
    {
      "epoch": 0.7208081022816732,
      "grad_norm": 0.08283010125160217,
      "learning_rate": 4.9362251262416505e-05,
      "loss": 0.0503,
      "step": 217000
    },
    {
      "epoch": 0.7224689504436126,
      "grad_norm": 0.08325310796499252,
      "learning_rate": 4.9359320411369325e-05,
      "loss": 0.0493,
      "step": 217500
    },
    {
      "epoch": 0.7241297986055519,
      "grad_norm": 0.08710219711065292,
      "learning_rate": 4.935638881020719e-05,
      "loss": 0.0499,
      "step": 218000
    },
    {
      "epoch": 0.7257906467674913,
      "grad_norm": 0.15566971898078918,
      "learning_rate": 4.935344470982402e-05,
      "loss": 0.0506,
      "step": 218500
    },
    {
      "epoch": 0.7274514949294305,
      "grad_norm": 0.12010032683610916,
      "learning_rate": 4.9350493979342125e-05,
      "loss": 0.0498,
      "step": 219000
    },
    {
      "epoch": 0.7291123430913699,
      "grad_norm": 0.08528698235750198,
      "learning_rate": 4.9347536619564824e-05,
      "loss": 0.0507,
      "step": 219500
    },
    {
      "epoch": 0.7307731912533092,
      "grad_norm": 0.149106964468956,
      "learning_rate": 4.9344572631297246e-05,
      "loss": 0.0501,
      "step": 220000
    },
    {
      "epoch": 0.7324340394152485,
      "grad_norm": 0.1534300297498703,
      "learning_rate": 4.934160201534633e-05,
      "loss": 0.0493,
      "step": 220500
    },
    {
      "epoch": 0.7340948875771879,
      "grad_norm": 0.07520589232444763,
      "learning_rate": 4.9338624772520795e-05,
      "loss": 0.0502,
      "step": 221000
    },
    {
      "epoch": 0.7357557357391272,
      "grad_norm": 0.05935743451118469,
      "learning_rate": 4.933564090363118e-05,
      "loss": 0.0501,
      "step": 221500
    },
    {
      "epoch": 0.7374165839010666,
      "grad_norm": 0.07157426327466965,
      "learning_rate": 4.9332656397089584e-05,
      "loss": 0.0485,
      "step": 222000
    },
    {
      "epoch": 0.7390774320630059,
      "grad_norm": 0.06202169880270958,
      "learning_rate": 4.932965929175872e-05,
      "loss": 0.0493,
      "step": 222500
    },
    {
      "epoch": 0.7407382802249453,
      "grad_norm": 0.08661901205778122,
      "learning_rate": 4.9326655562804584e-05,
      "loss": 0.0503,
      "step": 223000
    },
    {
      "epoch": 0.7423991283868846,
      "grad_norm": 0.08218231052160263,
      "learning_rate": 4.932364521104493e-05,
      "loss": 0.0485,
      "step": 223500
    },
    {
      "epoch": 0.7440599765488239,
      "grad_norm": 0.10484447330236435,
      "learning_rate": 4.9320634277855006e-05,
      "loss": 0.0502,
      "step": 224000
    },
    {
      "epoch": 0.7457208247107633,
      "grad_norm": 0.08731351792812347,
      "learning_rate": 4.9317610696186284e-05,
      "loss": 0.0496,
      "step": 224500
    },
    {
      "epoch": 0.7473816728727026,
      "grad_norm": 0.11383998394012451,
      "learning_rate": 4.931458049417447e-05,
      "loss": 0.0491,
      "step": 225000
    },
    {
      "epoch": 0.749042521034642,
      "grad_norm": 0.06423246115446091,
      "learning_rate": 4.93115436726445e-05,
      "loss": 0.0497,
      "step": 225500
    },
    {
      "epoch": 0.7507033691965813,
      "grad_norm": 0.06885494291782379,
      "learning_rate": 4.93085063259085e-05,
      "loss": 0.0503,
      "step": 226000
    },
    {
      "epoch": 0.7523642173585207,
      "grad_norm": 0.14932693541049957,
      "learning_rate": 4.9305462387752995e-05,
      "loss": 0.0498,
      "step": 226500
    },
    {
      "epoch": 0.75402506552046,
      "grad_norm": 0.09708664566278458,
      "learning_rate": 4.930240573910283e-05,
      "loss": 0.0497,
      "step": 227000
    },
    {
      "epoch": 0.7556859136823993,
      "grad_norm": 0.06048279255628586,
      "learning_rate": 4.929934247424904e-05,
      "loss": 0.0494,
      "step": 227500
    },
    {
      "epoch": 0.7573467618443387,
      "grad_norm": 0.13617730140686035,
      "learning_rate": 4.929627259402557e-05,
      "loss": 0.0492,
      "step": 228000
    },
    {
      "epoch": 0.759007610006278,
      "grad_norm": 0.0948338583111763,
      "learning_rate": 4.9293196099268196e-05,
      "loss": 0.049,
      "step": 228500
    },
    {
      "epoch": 0.7606684581682174,
      "grad_norm": 0.11938588321208954,
      "learning_rate": 4.929011299081446e-05,
      "loss": 0.0495,
      "step": 229000
    },
    {
      "epoch": 0.7623293063301567,
      "grad_norm": 0.0784769058227539,
      "learning_rate": 4.928702326950374e-05,
      "loss": 0.05,
      "step": 229500
    },
    {
      "epoch": 0.7639901544920961,
      "grad_norm": 0.054426100105047226,
      "learning_rate": 4.928393313544206e-05,
      "loss": 0.0493,
      "step": 230000
    },
    {
      "epoch": 0.7656510026540354,
      "grad_norm": 0.11944360285997391,
      "learning_rate": 4.928083020416414e-05,
      "loss": 0.0488,
      "step": 230500
    },
    {
      "epoch": 0.7673118508159747,
      "grad_norm": 0.11839602887630463,
      "learning_rate": 4.92777206625564e-05,
      "loss": 0.05,
      "step": 231000
    },
    {
      "epoch": 0.7689726989779141,
      "grad_norm": 0.060225024819374084,
      "learning_rate": 4.9274604511465424e-05,
      "loss": 0.0505,
      "step": 231500
    },
    {
      "epoch": 0.7706335471398533,
      "grad_norm": 0.12124183773994446,
      "learning_rate": 4.9271481751739564e-05,
      "loss": 0.0497,
      "step": 232000
    },
    {
      "epoch": 0.7722943953017927,
      "grad_norm": 0.0682835653424263,
      "learning_rate": 4.9268358649557986e-05,
      "loss": 0.0498,
      "step": 232500
    },
    {
      "epoch": 0.773955243463732,
      "grad_norm": 0.08085819333791733,
      "learning_rate": 4.9265222688327614e-05,
      "loss": 0.0495,
      "step": 233000
    },
    {
      "epoch": 0.7756160916256714,
      "grad_norm": 0.09007267653942108,
      "learning_rate": 4.926208012101651e-05,
      "loss": 0.0492,
      "step": 233500
    },
    {
      "epoch": 0.7772769397876107,
      "grad_norm": 0.06427103281021118,
      "learning_rate": 4.9258930948480205e-05,
      "loss": 0.0492,
      "step": 234000
    },
    {
      "epoch": 0.77893778794955,
      "grad_norm": 0.06869034469127655,
      "learning_rate": 4.9255775171576066e-05,
      "loss": 0.0498,
      "step": 234500
    },
    {
      "epoch": 0.7805986361114894,
      "grad_norm": 0.08149431645870209,
      "learning_rate": 4.925261279116322e-05,
      "loss": 0.0495,
      "step": 235000
    },
    {
      "epoch": 0.7822594842734287,
      "grad_norm": 0.08704913407564163,
      "learning_rate": 4.9249443808102616e-05,
      "loss": 0.0487,
      "step": 235500
    },
    {
      "epoch": 0.7839203324353681,
      "grad_norm": 0.055085189640522,
      "learning_rate": 4.924627458101469e-05,
      "loss": 0.0504,
      "step": 236000
    },
    {
      "epoch": 0.7855811805973074,
      "grad_norm": 0.04452097415924072,
      "learning_rate": 4.924309240844957e-05,
      "loss": 0.0497,
      "step": 236500
    },
    {
      "epoch": 0.7872420287592468,
      "grad_norm": 0.13661853969097137,
      "learning_rate": 4.923990363582855e-05,
      "loss": 0.05,
      "step": 237000
    },
    {
      "epoch": 0.7889028769211861,
      "grad_norm": 0.0563708171248436,
      "learning_rate": 4.923670826401978e-05,
      "loss": 0.0487,
      "step": 237500
    },
    {
      "epoch": 0.7905637250831254,
      "grad_norm": 0.216948002576828,
      "learning_rate": 4.9233506293893184e-05,
      "loss": 0.0509,
      "step": 238000
    },
    {
      "epoch": 0.7922245732450648,
      "grad_norm": 0.07669079303741455,
      "learning_rate": 4.923029772632046e-05,
      "loss": 0.0498,
      "step": 238500
    },
    {
      "epoch": 0.7938854214070041,
      "grad_norm": 0.07490922510623932,
      "learning_rate": 4.922708256217514e-05,
      "loss": 0.0493,
      "step": 239000
    },
    {
      "epoch": 0.7955462695689435,
      "grad_norm": 0.08372893929481506,
      "learning_rate": 4.9223867252434146e-05,
      "loss": 0.0494,
      "step": 239500
    },
    {
      "epoch": 0.7972071177308828,
      "grad_norm": 0.09422881156206131,
      "learning_rate": 4.922063891096013e-05,
      "loss": 0.0499,
      "step": 240000
    },
    {
      "epoch": 0.7988679658928222,
      "grad_norm": 0.12796878814697266,
      "learning_rate": 4.921740397554307e-05,
      "loss": 0.0481,
      "step": 240500
    },
    {
      "epoch": 0.8005288140547615,
      "grad_norm": 0.09627208858728409,
      "learning_rate": 4.9214162447063674e-05,
      "loss": 0.0494,
      "step": 241000
    },
    {
      "epoch": 0.8021896622167008,
      "grad_norm": 0.0964161604642868,
      "learning_rate": 4.921091432640442e-05,
      "loss": 0.0501,
      "step": 241500
    },
    {
      "epoch": 0.8038505103786402,
      "grad_norm": 0.04854239523410797,
      "learning_rate": 4.92076596144496e-05,
      "loss": 0.0503,
      "step": 242000
    },
    {
      "epoch": 0.8055113585405795,
      "grad_norm": 0.07209300994873047,
      "learning_rate": 4.9204398312085285e-05,
      "loss": 0.0488,
      "step": 242500
    },
    {
      "epoch": 0.8071722067025189,
      "grad_norm": 0.089186891913414,
      "learning_rate": 4.920113042019936e-05,
      "loss": 0.0502,
      "step": 243000
    },
    {
      "epoch": 0.8088330548644582,
      "grad_norm": 0.07419790327548981,
      "learning_rate": 4.919786249521739e-05,
      "loss": 0.0491,
      "step": 243500
    },
    {
      "epoch": 0.8104939030263976,
      "grad_norm": 0.11937033385038376,
      "learning_rate": 4.91945814401336e-05,
      "loss": 0.0488,
      "step": 244000
    },
    {
      "epoch": 0.8121547511883369,
      "grad_norm": 0.10335565358400345,
      "learning_rate": 4.91912937982008e-05,
      "loss": 0.0495,
      "step": 244500
    },
    {
      "epoch": 0.8138155993502763,
      "grad_norm": 0.06519521772861481,
      "learning_rate": 4.9187999570314014e-05,
      "loss": 0.0495,
      "step": 245000
    },
    {
      "epoch": 0.8154764475122155,
      "grad_norm": 0.11343563348054886,
      "learning_rate": 4.9184705365567254e-05,
      "loss": 0.0507,
      "step": 245500
    },
    {
      "epoch": 0.8171372956741548,
      "grad_norm": 0.053650569170713425,
      "learning_rate": 4.9181397981632224e-05,
      "loss": 0.0492,
      "step": 246000
    },
    {
      "epoch": 0.8187981438360942,
      "grad_norm": 0.1024019718170166,
      "learning_rate": 4.9178084014437296e-05,
      "loss": 0.0487,
      "step": 246500
    },
    {
      "epoch": 0.8204589919980335,
      "grad_norm": 0.054280880838632584,
      "learning_rate": 4.917476346488469e-05,
      "loss": 0.0495,
      "step": 247000
    },
    {
      "epoch": 0.8221198401599729,
      "grad_norm": 0.10193145275115967,
      "learning_rate": 4.917144299470808e-05,
      "loss": 0.0507,
      "step": 247500
    },
    {
      "epoch": 0.8237806883219122,
      "grad_norm": 0.05546634644269943,
      "learning_rate": 4.916810929631409e-05,
      "loss": 0.0488,
      "step": 248000
    },
    {
      "epoch": 0.8254415364838515,
      "grad_norm": 0.08817209303379059,
      "learning_rate": 4.916476901827797e-05,
      "loss": 0.0485,
      "step": 248500
    },
    {
      "epoch": 0.8271023846457909,
      "grad_norm": 0.06557662039995193,
      "learning_rate": 4.91614221615091e-05,
      "loss": 0.0489,
      "step": 249000
    },
    {
      "epoch": 0.8287632328077302,
      "grad_norm": 0.09304113686084747,
      "learning_rate": 4.915807544035189e-05,
      "loss": 0.0493,
      "step": 249500
    },
    {
      "epoch": 0.8304240809696696,
      "grad_norm": 0.0708131343126297,
      "learning_rate": 4.915471544200571e-05,
      "loss": 0.0488,
      "step": 250000
    },
    {
      "epoch": 0.8320849291316089,
      "grad_norm": 0.09421884268522263,
      "learning_rate": 4.915134886766381e-05,
      "loss": 0.0488,
      "step": 250500
    },
    {
      "epoch": 0.8337457772935483,
      "grad_norm": 0.10723264515399933,
      "learning_rate": 4.914797571824274e-05,
      "loss": 0.0499,
      "step": 251000
    },
    {
      "epoch": 0.8354066254554876,
      "grad_norm": 0.09011033922433853,
      "learning_rate": 4.914460276066838e-05,
      "loss": 0.0502,
      "step": 251500
    },
    {
      "epoch": 0.837067473617427,
      "grad_norm": 0.09231589734554291,
      "learning_rate": 4.9141216476991256e-05,
      "loss": 0.0496,
      "step": 252000
    },
    {
      "epoch": 0.8387283217793663,
      "grad_norm": 0.10689172148704529,
      "learning_rate": 4.913782362099344e-05,
      "loss": 0.0482,
      "step": 252500
    },
    {
      "epoch": 0.8403891699413056,
      "grad_norm": 0.07304712384939194,
      "learning_rate": 4.913442419359863e-05,
      "loss": 0.0506,
      "step": 253000
    },
    {
      "epoch": 0.842050018103245,
      "grad_norm": 0.09386525303125381,
      "learning_rate": 4.913102501428474e-05,
      "loss": 0.0487,
      "step": 253500
    },
    {
      "epoch": 0.8437108662651843,
      "grad_norm": 0.06564856320619583,
      "learning_rate": 4.91276124600123e-05,
      "loss": 0.0494,
      "step": 254000
    },
    {
      "epoch": 0.8453717144271237,
      "grad_norm": 0.09265965968370438,
      "learning_rate": 4.912420018192342e-05,
      "loss": 0.0495,
      "step": 254500
    },
    {
      "epoch": 0.847032562589063,
      "grad_norm": 0.13177870213985443,
      "learning_rate": 4.9120774504482125e-05,
      "loss": 0.0498,
      "step": 255000
    },
    {
      "epoch": 0.8486934107510024,
      "grad_norm": 0.08258975297212601,
      "learning_rate": 4.911734226028535e-05,
      "loss": 0.0494,
      "step": 255500
    },
    {
      "epoch": 0.8503542589129417,
      "grad_norm": 0.0749736800789833,
      "learning_rate": 4.9113903450267515e-05,
      "loss": 0.0497,
      "step": 256000
    },
    {
      "epoch": 0.852015107074881,
      "grad_norm": 0.14627480506896973,
      "learning_rate": 4.9110458075364826e-05,
      "loss": 0.0491,
      "step": 256500
    },
    {
      "epoch": 0.8536759552368204,
      "grad_norm": 0.11904363334178925,
      "learning_rate": 4.9107006136515256e-05,
      "loss": 0.0493,
      "step": 257000
    },
    {
      "epoch": 0.8553368033987597,
      "grad_norm": 0.1087312251329422,
      "learning_rate": 4.9103554558211554e-05,
      "loss": 0.0487,
      "step": 257500
    },
    {
      "epoch": 0.8569976515606991,
      "grad_norm": 0.09061150997877121,
      "learning_rate": 4.910008950741253e-05,
      "loss": 0.0489,
      "step": 258000
    },
    {
      "epoch": 0.8586584997226383,
      "grad_norm": 0.09872207790613174,
      "learning_rate": 4.909661789548941e-05,
      "loss": 0.0496,
      "step": 258500
    },
    {
      "epoch": 0.8603193478845776,
      "grad_norm": 0.06986021250486374,
      "learning_rate": 4.909313972338734e-05,
      "loss": 0.0491,
      "step": 259000
    },
    {
      "epoch": 0.861980196046517,
      "grad_norm": 0.06876754760742188,
      "learning_rate": 4.9089654992053214e-05,
      "loss": 0.05,
      "step": 259500
    },
    {
      "epoch": 0.8636410442084563,
      "grad_norm": 0.0700400173664093,
      "learning_rate": 4.908616370243575e-05,
      "loss": 0.0492,
      "step": 260000
    },
    {
      "epoch": 0.8653018923703957,
      "grad_norm": 0.07983119785785675,
      "learning_rate": 4.908266585548542e-05,
      "loss": 0.049,
      "step": 260500
    },
    {
      "epoch": 0.866962740532335,
      "grad_norm": 0.061430830508470535,
      "learning_rate": 4.90791684675038e-05,
      "loss": 0.0495,
      "step": 261000
    },
    {
      "epoch": 0.8686235886942744,
      "grad_norm": 0.08114965260028839,
      "learning_rate": 4.907565752185626e-05,
      "loss": 0.0492,
      "step": 261500
    },
    {
      "epoch": 0.8702844368562137,
      "grad_norm": 0.05972182750701904,
      "learning_rate": 4.9072140021736095e-05,
      "loss": 0.0492,
      "step": 262000
    },
    {
      "epoch": 0.871945285018153,
      "grad_norm": 0.0825042799115181,
      "learning_rate": 4.906861596810095e-05,
      "loss": 0.049,
      "step": 262500
    },
    {
      "epoch": 0.8736061331800924,
      "grad_norm": 0.06705371290445328,
      "learning_rate": 4.9065085361910234e-05,
      "loss": 0.0499,
      "step": 263000
    },
    {
      "epoch": 0.8752669813420317,
      "grad_norm": 0.07805630564689636,
      "learning_rate": 4.906154820412512e-05,
      "loss": 0.0495,
      "step": 263500
    },
    {
      "epoch": 0.8769278295039711,
      "grad_norm": 0.10082937777042389,
      "learning_rate": 4.9058004495708585e-05,
      "loss": 0.0485,
      "step": 264000
    },
    {
      "epoch": 0.8785886776659104,
      "grad_norm": 0.0876896008849144,
      "learning_rate": 4.905445423762539e-05,
      "loss": 0.0492,
      "step": 264500
    },
    {
      "epoch": 0.8802495258278498,
      "grad_norm": 0.11711294203996658,
      "learning_rate": 4.905090455099061e-05,
      "loss": 0.0495,
      "step": 265000
    },
    {
      "epoch": 0.8819103739897891,
      "grad_norm": 0.09757383167743683,
      "learning_rate": 4.904734120956998e-05,
      "loss": 0.0491,
      "step": 265500
    },
    {
      "epoch": 0.8835712221517285,
      "grad_norm": 0.07719319313764572,
      "learning_rate": 4.904377132138572e-05,
      "loss": 0.0499,
      "step": 266000
    },
    {
      "epoch": 0.8852320703136678,
      "grad_norm": 0.08988476544618607,
      "learning_rate": 4.904019488740971e-05,
      "loss": 0.0492,
      "step": 266500
    },
    {
      "epoch": 0.8868929184756071,
      "grad_norm": 0.1073351800441742,
      "learning_rate": 4.9036619081104275e-05,
      "loss": 0.0487,
      "step": 267000
    },
    {
      "epoch": 0.8885537666375465,
      "grad_norm": 0.10349424928426743,
      "learning_rate": 4.903302957155424e-05,
      "loss": 0.0494,
      "step": 267500
    },
    {
      "epoch": 0.8902146147994858,
      "grad_norm": 0.08173850178718567,
      "learning_rate": 4.9029433519136836e-05,
      "loss": 0.0499,
      "step": 268000
    },
    {
      "epoch": 0.8918754629614252,
      "grad_norm": 0.10567450523376465,
      "learning_rate": 4.9025830924831076e-05,
      "loss": 0.0494,
      "step": 268500
    },
    {
      "epoch": 0.8935363111233645,
      "grad_norm": 0.07688502967357635,
      "learning_rate": 4.9022229014415334e-05,
      "loss": 0.0501,
      "step": 269000
    },
    {
      "epoch": 0.8951971592853039,
      "grad_norm": 0.09498882293701172,
      "learning_rate": 4.901862059020617e-05,
      "loss": 0.0486,
      "step": 269500
    },
    {
      "epoch": 0.8968580074472432,
      "grad_norm": 0.12725867331027985,
      "learning_rate": 4.901499840228096e-05,
      "loss": 0.0492,
      "step": 270000
    },
    {
      "epoch": 0.8985188556091825,
      "grad_norm": 0.07609894126653671,
      "learning_rate": 4.901136967639728e-05,
      "loss": 0.0493,
      "step": 270500
    },
    {
      "epoch": 0.9001797037711219,
      "grad_norm": 0.10549204796552658,
      "learning_rate": 4.900773441354303e-05,
      "loss": 0.0491,
      "step": 271000
    },
    {
      "epoch": 0.9018405519330612,
      "grad_norm": 0.11764267832040787,
      "learning_rate": 4.9004092614707875e-05,
      "loss": 0.05,
      "step": 271500
    },
    {
      "epoch": 0.9035014000950005,
      "grad_norm": 0.06065346673130989,
      "learning_rate": 4.900044428088329e-05,
      "loss": 0.0493,
      "step": 272000
    },
    {
      "epoch": 0.9051622482569398,
      "grad_norm": 0.091370590031147,
      "learning_rate": 4.8996789413062516e-05,
      "loss": 0.049,
      "step": 272500
    },
    {
      "epoch": 0.9068230964188791,
      "grad_norm": 0.08665766566991806,
      "learning_rate": 4.899312801224056e-05,
      "loss": 0.0493,
      "step": 273000
    },
    {
      "epoch": 0.9084839445808185,
      "grad_norm": 0.10977195203304291,
      "learning_rate": 4.898946742179817e-05,
      "loss": 0.0485,
      "step": 273500
    },
    {
      "epoch": 0.9101447927427578,
      "grad_norm": 0.11251412332057953,
      "learning_rate": 4.898579297102706e-05,
      "loss": 0.0486,
      "step": 274000
    },
    {
      "epoch": 0.9118056409046972,
      "grad_norm": 0.07261673361063004,
      "learning_rate": 4.898211935872633e-05,
      "loss": 0.0484,
      "step": 274500
    },
    {
      "epoch": 0.9134664890666365,
      "grad_norm": 0.08516756445169449,
      "learning_rate": 4.8978431861999454e-05,
      "loss": 0.049,
      "step": 275000
    },
    {
      "epoch": 0.9151273372285759,
      "grad_norm": 0.07198826223611832,
      "learning_rate": 4.897473783726915e-05,
      "loss": 0.0489,
      "step": 275500
    },
    {
      "epoch": 0.9167881853905152,
      "grad_norm": 0.1205843836069107,
      "learning_rate": 4.89710372855411e-05,
      "loss": 0.0495,
      "step": 276000
    },
    {
      "epoch": 0.9184490335524546,
      "grad_norm": 0.06285624951124191,
      "learning_rate": 4.8967330207822756e-05,
      "loss": 0.0494,
      "step": 276500
    },
    {
      "epoch": 0.9201098817143939,
      "grad_norm": 0.10792280733585358,
      "learning_rate": 4.896361660512335e-05,
      "loss": 0.0491,
      "step": 277000
    },
    {
      "epoch": 0.9217707298763332,
      "grad_norm": 0.07120498269796371,
      "learning_rate": 4.895989647845391e-05,
      "loss": 0.049,
      "step": 277500
    },
    {
      "epoch": 0.9234315780382726,
      "grad_norm": 0.10482166707515717,
      "learning_rate": 4.895616982882719e-05,
      "loss": 0.0486,
      "step": 278000
    },
    {
      "epoch": 0.9250924262002119,
      "grad_norm": 0.0794406607747078,
      "learning_rate": 4.895244413010913e-05,
      "loss": 0.0496,
      "step": 278500
    },
    {
      "epoch": 0.9267532743621513,
      "grad_norm": 0.10987851768732071,
      "learning_rate": 4.894870445065418e-05,
      "loss": 0.049,
      "step": 279000
    },
    {
      "epoch": 0.9284141225240906,
      "grad_norm": 0.07316169142723083,
      "learning_rate": 4.8944958251288944e-05,
      "loss": 0.0492,
      "step": 279500
    },
    {
      "epoch": 0.93007497068603,
      "grad_norm": 0.14542104303836823,
      "learning_rate": 4.89412055330333e-05,
      "loss": 0.0494,
      "step": 280000
    },
    {
      "epoch": 0.9317358188479693,
      "grad_norm": 0.09514959901571274,
      "learning_rate": 4.893745382188529e-05,
      "loss": 0.0485,
      "step": 280500
    },
    {
      "epoch": 0.9333966670099086,
      "grad_norm": 0.1393292099237442,
      "learning_rate": 4.893368808194824e-05,
      "loss": 0.0497,
      "step": 281000
    },
    {
      "epoch": 0.935057515171848,
      "grad_norm": 0.07890672236680984,
      "learning_rate": 4.892991582618902e-05,
      "loss": 0.0493,
      "step": 281500
    },
    {
      "epoch": 0.9367183633337873,
      "grad_norm": 0.0842231810092926,
      "learning_rate": 4.8926137055634615e-05,
      "loss": 0.0497,
      "step": 282000
    },
    {
      "epoch": 0.9383792114957267,
      "grad_norm": 0.09783921390771866,
      "learning_rate": 4.892235934838246e-05,
      "loss": 0.0487,
      "step": 282500
    },
    {
      "epoch": 0.940040059657666,
      "grad_norm": 0.07999075204133987,
      "learning_rate": 4.891856756435015e-05,
      "loss": 0.0493,
      "step": 283000
    },
    {
      "epoch": 0.9417009078196054,
      "grad_norm": 0.11091974377632141,
      "learning_rate": 4.8914769268612155e-05,
      "loss": 0.0486,
      "step": 283500
    },
    {
      "epoch": 0.9433617559815447,
      "grad_norm": 0.08099398761987686,
      "learning_rate": 4.891096446220254e-05,
      "loss": 0.0486,
      "step": 284000
    },
    {
      "epoch": 0.945022604143484,
      "grad_norm": 0.14362426102161407,
      "learning_rate": 4.8907160775285166e-05,
      "loss": 0.0489,
      "step": 284500
    },
    {
      "epoch": 0.9466834523054233,
      "grad_norm": 0.06798277795314789,
      "learning_rate": 4.8903342963657774e-05,
      "loss": 0.0484,
      "step": 285000
    },
    {
      "epoch": 0.9483443004673626,
      "grad_norm": 0.06750600785017014,
      "learning_rate": 4.889951864446951e-05,
      "loss": 0.0494,
      "step": 285500
    },
    {
      "epoch": 0.950005148629302,
      "grad_norm": 0.08664051443338394,
      "learning_rate": 4.889568781876154e-05,
      "loss": 0.049,
      "step": 286000
    },
    {
      "epoch": 0.9516659967912413,
      "grad_norm": 0.1011972427368164,
      "learning_rate": 4.889185816873091e-05,
      "loss": 0.0495,
      "step": 286500
    },
    {
      "epoch": 0.9533268449531807,
      "grad_norm": 0.131114199757576,
      "learning_rate": 4.888801434612187e-05,
      "loss": 0.0482,
      "step": 287000
    },
    {
      "epoch": 0.95498769311512,
      "grad_norm": 0.05310574173927307,
      "learning_rate": 4.88841640201251e-05,
      "loss": 0.0477,
      "step": 287500
    },
    {
      "epoch": 0.9566485412770593,
      "grad_norm": 0.07235179096460342,
      "learning_rate": 4.888030719178883e-05,
      "loss": 0.0494,
      "step": 288000
    },
    {
      "epoch": 0.9583093894389987,
      "grad_norm": 0.0425499752163887,
      "learning_rate": 4.887645159530991e-05,
      "loss": 0.0498,
      "step": 288500
    },
    {
      "epoch": 0.959970237600938,
      "grad_norm": 0.05063682049512863,
      "learning_rate": 4.887258177844585e-05,
      "loss": 0.0481,
      "step": 289000
    },
    {
      "epoch": 0.9616310857628774,
      "grad_norm": 0.08897846937179565,
      "learning_rate": 4.8868705462395505e-05,
      "loss": 0.0481,
      "step": 289500
    },
    {
      "epoch": 0.9632919339248167,
      "grad_norm": 0.12234726548194885,
      "learning_rate": 4.886482264821416e-05,
      "loss": 0.0489,
      "step": 290000
    },
    {
      "epoch": 0.9649527820867561,
      "grad_norm": 0.05191543325781822,
      "learning_rate": 4.8860941122064804e-05,
      "loss": 0.0488,
      "step": 290500
    },
    {
      "epoch": 0.9666136302486954,
      "grad_norm": 0.09610164165496826,
      "learning_rate": 4.8857045327785474e-05,
      "loss": 0.0482,
      "step": 291000
    },
    {
      "epoch": 0.9682744784106347,
      "grad_norm": 0.0939910039305687,
      "learning_rate": 4.8853143038549566e-05,
      "loss": 0.0502,
      "step": 291500
    },
    {
      "epoch": 0.9699353265725741,
      "grad_norm": 0.09055228531360626,
      "learning_rate": 4.884923425541946e-05,
      "loss": 0.0497,
      "step": 292000
    },
    {
      "epoch": 0.9715961747345134,
      "grad_norm": 0.06538115441799164,
      "learning_rate": 4.884532681649036e-05,
      "loss": 0.0499,
      "step": 292500
    },
    {
      "epoch": 0.9732570228964528,
      "grad_norm": 0.1004188060760498,
      "learning_rate": 4.8841405061748536e-05,
      "loss": 0.0481,
      "step": 293000
    },
    {
      "epoch": 0.9749178710583921,
      "grad_norm": 0.06237140670418739,
      "learning_rate": 4.883747681630812e-05,
      "loss": 0.049,
      "step": 293500
    },
    {
      "epoch": 0.9765787192203315,
      "grad_norm": 0.10366128385066986,
      "learning_rate": 4.883354208123855e-05,
      "loss": 0.0501,
      "step": 294000
    },
    {
      "epoch": 0.9782395673822708,
      "grad_norm": 0.10709209740161896,
      "learning_rate": 4.8829608746533165e-05,
      "loss": 0.0485,
      "step": 294500
    },
    {
      "epoch": 0.9799004155442101,
      "grad_norm": 0.06068003550171852,
      "learning_rate": 4.882566895026468e-05,
      "loss": 0.0487,
      "step": 295000
    },
    {
      "epoch": 0.9815612637061495,
      "grad_norm": 0.0662832111120224,
      "learning_rate": 4.88217147786855e-05,
      "loss": 0.0491,
      "step": 295500
    },
    {
      "epoch": 0.9832221118680888,
      "grad_norm": 0.06661573797464371,
      "learning_rate": 4.88177541217683e-05,
      "loss": 0.0484,
      "step": 296000
    },
    {
      "epoch": 0.9848829600300282,
      "grad_norm": 0.08625780791044235,
      "learning_rate": 4.881378698059136e-05,
      "loss": 0.0488,
      "step": 296500
    },
    {
      "epoch": 0.9865438081919675,
      "grad_norm": 0.08052616566419601,
      "learning_rate": 4.88098133562347e-05,
      "loss": 0.0486,
      "step": 297000
    },
    {
      "epoch": 0.9882046563539069,
      "grad_norm": 0.10975775867700577,
      "learning_rate": 4.880583324978013e-05,
      "loss": 0.0496,
      "step": 297500
    },
    {
      "epoch": 0.9898655045158462,
      "grad_norm": 0.07190445810556412,
      "learning_rate": 4.880184666231121e-05,
      "loss": 0.0487,
      "step": 298000
    },
    {
      "epoch": 0.9915263526777854,
      "grad_norm": 0.06178111582994461,
      "learning_rate": 4.8797853594913266e-05,
      "loss": 0.0485,
      "step": 298500
    },
    {
      "epoch": 0.9931872008397248,
      "grad_norm": 0.10553804785013199,
      "learning_rate": 4.879386205423103e-05,
      "loss": 0.0484,
      "step": 299000
    },
    {
      "epoch": 0.9948480490016641,
      "grad_norm": 0.09341724961996078,
      "learning_rate": 4.878985604319249e-05,
      "loss": 0.0493,
      "step": 299500
    },
    {
      "epoch": 0.9965088971636035,
      "grad_norm": 0.13837555050849915,
      "learning_rate": 4.8785843555489324e-05,
      "loss": 0.0494,
      "step": 300000
    },
    {
      "epoch": 0.9981697453255428,
      "grad_norm": 0.0628712996840477,
      "learning_rate": 4.8781824592213886e-05,
      "loss": 0.0487,
      "step": 300500
    },
    {
      "epoch": 0.9998305934874822,
      "grad_norm": 0.12860123813152313,
      "learning_rate": 4.877780721179664e-05,
      "loss": 0.0482,
      "step": 301000
    },
    {
      "epoch": 1.0,
      "eval_bleu": 3.969887792866861,
      "eval_loss": 0.045076675713062286,
      "eval_meteor": 0.40040647558230064,
      "eval_runtime": 154282.5271,
      "eval_samples_per_second": 6.758,
      "eval_steps_per_second": 0.241,
      "step": 301051
    }
  ],
  "logging_steps": 500,
  "max_steps": 3010510,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.852132897119273e+17,
  "train_batch_size": 28,
  "trial_name": null,
  "trial_params": null
}
